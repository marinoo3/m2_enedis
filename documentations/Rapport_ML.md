# Rapport sur les mod√®les de Machine Learning employ√©s lors du projet 

## 1. Introduction et Contexte

### 1.1 Objectif du projet

Ce projet vise √† d√©velopper une application web permettant de **pr√©dire la performance √©nerg√©tique des logements** en Haute-Savoie (d√©partement 74) ainsi qu'essayer de pr√©dire le Diagnostic de Performance √ânerg√©tique (DPE). L'application doit fournir deux types de pr√©dictions :

1. **R√©gression** : Estimation de la consommation √©nerg√©tique annuelle kWhep/an
2. **Classification** : D√©tection des "passoires √©nerg√©tiques" (logements class√©s F ou G)

### 1.2 Contraintes du projet

L'application web impose d'utiliser un mod√®le de machine learning avec des contraintes sp√©cifiques :

- **Simplicit√© d'utilisation** : Difficile de demander trop d'informations techniques √† l'utilisateur
- **Rapidit√© de saisie** : Limiter ainsi le nombre de questions pos√©es √† l'utilisateur
- **Accessibilit√© des donn√©es** : Utiliser uniquement des informations que l'utilisateur conna√Æt sans expertise technique ou les apporter via des calculs simples
- **Performance pr√©dictive** : Maintenir une bonne pr√©cision malgr√© la r√©duction des features

### 1.3 Dataset

<!-- A faire revoir par Marin sur les pr√©cisions √† apporter -->
- **Source** : `logements_74.csv` - Donn√©es issues de l'API de l'ADEME (Agence de la transition √©cologique) enrichies avec des donn√©es g√©ographiques
- **Taille** : 203 792 observations (logements)
- **Variables initiales** : 236 colonnes
- **Cible r√©gression** : `conso_5_usages_ep` (consommation √©nerg√©tique en kWhep/an)
- **Cible classification** : `passoire` (variable binaire : 1 si DPE F/G, 0 sinon)
---

## 2. M√©thodologie de construction des mod√®les

### 2.1 S√©lection des variables

La s√©lection des features a √©t√© r√©alis√©e en deux phases distinctes.

#### **Phase 1 : S√©lection statistique (36 features)**

Dans un premier temps, avec un jeu de donn√©es contenant 236 variables, une premi√®re s√©l√©ction "th√©orique" a √©t√© effectu√©e en utilisant des m√©thodes statistiques classiques
ce qui a permis de r√©duire le nombre de variables √† 36. certaines variables utilis√©s directement ou indirectement pour le calcul du DPE on √©t√© √©cart√©s √©galement afin d'√©viter le **data leakage**.
ensuite, ces 36 variables ont servi de base pour √©tablir une performance de r√©f√©rence pour les mod√®les.
une fois cette √©tape r√©alis√©e, les variables ont √©t√© r√©duites d'avantage pour se limiter a des features plus "user-friendly" pour l'application web.

**a) Analyse de corr√©lation**
- **Seuil appliqu√©** : |r| > 0.10 avec la cible
- **Objectif** : Identifier les variables lin√©airement li√©es √† la consommation √©nerg√©tique
- Variables hautement corr√©l√©es : isolation, √©nergie de chauffage, surface, p√©riode de construction

![Graphique des 25 variables quantitatives les plus fortement correl√©s avec la consommation energetique](plot_corr.png)

**b) Test du Chi-2 pour variables cat√©gorielles**
- **Test statistique** : Chi-2 d'ind√©pendance
- **Seuil de significativit√©** : p-value < 0.05
- **Objectif** : Valider la d√©pendance statistique entre variables qualitatives et la cible
- Exemples : type de b√¢timent, zone climatique, type d'√©nergie

![Graphique des 25 variables qualitatives avec le score de chi-2 le plus √©lev√©s pour la pr√©diction de passoire](plot_chi2.png)


**c) Calcul du VIF (Variance Inflation Factor)**
- **Objectif** : √âliminer la multicolin√©arit√© entre pr√©dicteurs, puisque beaucoup des variables du jeux de donn√©es sont interconnect√©s.
- **Seuil appliqu√©** : VIF < 10
- **R√©sultat** : Exclusion de variables redondantes (ex : 'surface_chauffee_installation_chauffage_n1',
'surface_habitable_desservie_par_installation_ecs_n1'
qui sont dans la majeure partie des cas extremement proche de la surface du logement elle m√™me)

![Graphique du VIF des diff√©rentes variables](plot_VIF.png)

**R√©sultat Phase 1** : 36 features s√©lectionn√©es, performance de r√©f√©rence √©tablie

#### **Phase 2 : R√©duction suppl√©mentaire des variables (9 features)**

Pour l'application web, une s√©lection plus drastique a √©t√© n√©cessaire en privil√©giant :

**Crit√®res de s√©lection** :
1. **Compr√©hensibilit√©** : Variable compr√©hensible par un utilisateur non-expert
2. **Disponibilit√©** : Information connue sans mesure technique
3. **Impact pr√©dictif** : Contribution significative aux mod√®les (feature importance)
4. **Non-redondance** : Pas de duplication d'information

**Features retenues (9 au total)** :
| Feature | Type | Source | Exemple |
|---------|------|--------|---------|
| `surface_habitable_logement` | Num√©rique | Saisie utilisateur | 75.0 |
| `periode_construction` | Cat√©gorielle | Saisie utilisateur | `1975-1989` |
| `type_batiment` | Cat√©gorielle | Saisie utilisateur (type b√¢timent) | `appartement` |
| `qualite_isolation_enveloppe` | Cat√©gorielle | Saisie utilisateur | `moyenne` |
| `type_energie_principale_chauffage` | Cat√©gorielle | Saisie utilisateur | `electricite` |
| `logement_traversant` (`traversant`) | Binaire (oui/non) | Saisie utilisateur (optionnel) | `non` |
| `protection_solaire_exterieure` | Binaire (oui/non) | Saisie utilisateur (optionnel) | `oui` |
| `zone_climatique` | Cat√©gorielle | Auto-calcul√© (code postal) | `H1a` |
| `classe_altitude` | Cat√©gorielle | Auto-calcul√© (code postal) | `400-800` |

**Justification de la r√©duction** :
- R√©tention de **75%+ de la performance** en r√©gression
- Simplification du mod√®le (- temps d'entrainements / - temps de pr√©diction)
- √âlimination des barri√®res techniques √† l'utilisation

Pour chaque t√¢che (r√©gression et classification), plusieurs algorithmes ont √©t√© s√©lectionn√©s selon des crit√®res compl√©mentaires :

#### **R√©gression (pr√©diction de consommation)**

| Mod√®le | Justification | Avantages | Inconv√©nients |
|--------|---------------|-----------|---------------|
| **Ridge** | R√©gression lin√©aire r√©gularis√©e (L2) | Simple, rapide, interpr√©table | Assume lin√©arit√© |
| **Random Forest** | Ensemble de arbres de d√©cision | Robuste, g√®re non-lin√©arit√©s, feature importance | Moins interpr√©table |
| **Gradient Boosting** | Boosting s√©quentiel d'arbres faibles | Tr√®s performant, flexible | Risque de surapprentissage |
| **XGBoost** | Gradient Boosting optimis√© | Tr√®s performant, r√©gularisation int√©gr√©e, rapide | Complexe √† param√©trer |


#### **Classification (d√©tection passoires F/G)**

| Mod√®le | Justification | Avantages | Inconv√©nients |
|--------|---------------|-----------|---------------|
| **Logistic Regression** | Mod√®le lin√©aire probabiliste | Probabilit√©s calibr√©es, rapide | Lin√©aire uniquement |
| **Random Forest** | Ensemble d'arbres (classification) | Robuste aux d√©s√©quilibres (avec class_weight) | Bo√Æte noire |
| **Gradient Boosting** | Boosting pour classification binaire | Excellentes performances | Plus lent √† entra√Æner |
| **XGBoost** | Gradient Boosting optimis√© | Tr√®s performant, gestion du d√©s√©quilibre (scale_pos_weight), rapide | Complexe √† param√©trer |

**Justification du choix** :
- **Diversit√© algorithmique** : Approches lin√©aires vs arborescentes
- **Compl√©mentarit√©** : Chaque mod√®le capture diff√©rents patterns
- **Benchmark** : Permet de comparer et s√©lectionner le meilleur
- **Exclusion de SVM** : Non retenu en raison de temps de r√©-entra√Ænement trop √©lev√©s, incompatible avec les objectifs du projet


### 2.3 D√©finition des hyperparam√®tres

L'optimisation des hyperparam√®tres a √©t√© r√©alis√©e via **GridSearchCV** avec validation crois√©e (CV=5).
ayant pour objectif de garder un mod√®le simple, et avec un ordinateur dont les performances sont assez limit√©s, j'ai opt√© pour assez peu de param√®tres lors du GridSearch.

#### **R√©gression**

**Ridge** (5 combinaisons)
```python
{
    'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],  # Force de r√©gularisation
    'model__solver': ['auto']
}
```

**Random Forest** (2 combinaisons)
```python
{
    'model__n_estimators': [100],           # Nombre d'arbres (fix√©)
    'model__max_depth': [15, 20],           # Profondeur maximale
    'model__min_samples_split': [5],        # √âchantillons min pour split
    'model__min_samples_leaf': [2],         # √âchantillons min par feuille
    'model__max_features': ['sqrt']         # Features par arbre
}
```

**Gradient Boosting** (2 combinaisons)
```python
{
    'model__n_estimators': [50],            # Nombre d'it√©rations
    'model__learning_rate': [0.1],          # Taux d'apprentissage
    'model__max_depth': [3, 5],             # Profondeur des arbres
    'model__min_samples_split': [5]
}
```

**XGBoost** (8 combinaisons)
```python
{
    'model__n_estimators': [50],            # Nombre d'it√©rations
    'model__learning_rate': [0.1],          # Taux d'apprentissage
    'model__max_depth': [3, 5],             # Profondeur des arbres
    'model__min_child_weight': [1, 3],      # Poids minimum des √©chantillons
    'model__reg_alpha': [0.0, 0.1],         # R√©gularisation L1
    'model__reg_lambda': [1.0]              # R√©gularisation L2
}
```

#### **Classification**

**Logistic Regression** (3 combinaisons)
```python
{
    'model__C': [0.1, 1.0, 10.0],           # Inverse de la r√©gularisation
    'model__penalty': ['l2'],
    'model__solver': ['lbfgs'],
    'model__max_iter': [1000]
}
```

**Random Forest** (2 combinaisons)
```python
{
    'model__n_estimators': [100],
    'model__max_depth': [15, 20],
    'model__min_samples_split': [5],
    'model__min_samples_leaf': [2],
    'model__class_weight': ['balanced']     # Important pour d√©s√©quilibre
}
```

**Gradient Boosting** (4 combinaisons)
```python
{
    'model__n_estimators': [50],
    'model__learning_rate': [0.1],
    'model__max_depth': [3, 5],
    'model__min_samples_split': [5]
}
```

**XGBoost** (8 combinaisons)
```python
{
    'model__n_estimators': [50],            # Nombre d'it√©rations
    'model__learning_rate': [0.1],          # Taux d'apprentissage
    'model__max_depth': [3, 5],             # Profondeur des arbres
    'model__min_child_weight': [1, 3],      # Poids minimum des √©chantillons
    'model__reg_alpha': [0.0, 0.1],         # R√©gularisation L1
    'model__reg_lambda': [1.0]              # R√©gularisation L2
}
```

**Strat√©gie d'optimisation** :
- **GridSearchCV** : Recherche exhaustive sur grilles r√©duites
- **CV = 5 folds** : Validation crois√©e pour robustesse
- **Scoring** : 
  - R√©gression : `neg_mean_squared_error` (minimiser RMSE)
  - Classification : `roc_auc` (maximiser AUC-ROC)
- **Parall√©lisation** : `n_jobs=-1` pour acc√©l√©rer (param√®tres de Sci-kit learn, qui utilise joblib pour paralleliser plusieurs arbres lors d'une Random forest ou plusieurs param√®tres d'une Cross Validation par exemple)


### 2.4 Preprocessing et Pipeline

Un pipeline sklearn unifi√© a √©t√© construit pour assurer la reproductibilit√© :

```python
Pipeline([
    ('preprocessor', ColumnTransformer([
        ('num', StandardScaler(), numeric_features),        # Normalisation
        ('cat', OneHotEncoder(drop='first'), categorical_features)  # Encodage
    ])),
    ('model', [Ridge|RandomForest|GradientBoosting])
])
```

**Transformations appliqu√©es** :
- **Variables num√©riques** : Standardisation (moyenne=0, √©cart-type=1)
- **Variables cat√©gorielles** : One-Hot Encoding avec suppression de la premi√®re cat√©gorie (√©viter multicolin√©arit√©)

---

## 3. Pr√©sentation des mod√®les retenus et R√©sultats

### 3.1 R√©sultats de la Cross-Validation
#### **R√©gression (36 features - mod√®le de r√©f√©rence)**


##### **R√âSUM√â DES R√âSULTATS - R√âGRESSION**

          Mod√®le         RMSE          MAE       R¬≤
GradientBoosting 15859.252033  4491.280118 0.917968
         XGBoost 15939.436265  4503.246191 0.917137
    RandomForest 26427.912160  5212.191746 0.772206
           Ridge 39335.147997 10277.786640 0.495363

**‚Üí Gradient Boosting** montre la meilleure capacit√© de g√©n√©ralisation.

#### **R√©gression (9 features - mod√®le simplifi√©)**


##### **R√âSUM√â DES PERFORMANCES (USER-FRIENDLY)**


          Mod√®le R¬≤ Train R¬≤ Test RMSE Train RMSE Test MAE Train MAE Test Temps (s)
           Ridge   0.4646  0.4094     40,996    42,553    10,987   10,934      5.74
    RandomForest   0.6731  0.5924     32,034    35,351     6,007    6,544     25.87
GradientBoosting   0.6945  0.5883     30,967    35,530     6,703    6,915    157.53
         XGBoost   0.6131  0.5830     34,850    35,757     6,853    7,024     34.96
**‚Üí Random Forest** et **Gradient Boosting** restent performants malgr√© la r√©duction des features. Random Forest √©tant plus rapide √† entra√Æner ce sera le mod√®le retenu pour l'application web.

#### Comparatif des performances (36 vs 9 features)

![graphiques de comparaison des performances des deux mod√®les](plot_comparaison.png)


#### **Classification (10 features)**


##### **R√âSUM√â - MOD√àLES CLASSIFICATION (10 FEATURES)**

            Mod√®le  Accuracy  Precision   Recall  F1-Score  ROC-AUC  Temps (s)
      RandomForest  0.827253   0.455997 0.899983  0.605303 0.930999  54.286304
  GradientBoosting  0.895729   0.725911 0.468411  0.569402 0.928165 119.644875
           XGBoost  0.883339   0.591176 0.672279  0.629124 0.926439  62.221985
LogisticRegression  0.888221   0.670527 0.472912  0.554643 0.915158  10.153797

**‚Üí Random Forest** offre le meilleur compromis entre pr√©cision et rappel, crucial pour la d√©tection des passoires √©nerg√©tiques.

Gradient boosting est le mod√®le avec la meilleur accuracy, mais avec un recall faible (0.468) ce qui n'est pas adapt√© √† notre probl√©matique o√π l'on cherche √† d√©tecter un maximum de passoires √©nerg√©tiques.

XGBoost offre des performances interm√©diaires. avec le meilleur F1-score (0.629) il aurait pu √™tre un bon choix si l'on cherchait a prioriser le f1-score pour un mod√®le plus √©quilibr√© entre precision et recall.


![Graphique des performances des mod√®les de classification](plot_classification.png)

### 3.3 Analyse comparative

#### **Justification des choix de mod√®les**

Les deux t√¢ches ont conduit √† la s√©lection du m√™me algorithme optimis√© pour chaque contexte :

**üîπ R√©gression : Random Forest**

‚úÖ **Avantages** :
- Bonnes performances : R¬≤ = 0.592, RMSE = 35 351 kWhep/an
- Temps d'entra√Ænement rapide (~26s) compar√© √† Gradient Boosting (~158s)
- Robustesse aux donn√©es bruit√©es
- Capacit√© √† capturer des interactions non-lin√©aires
- Importance des features explicite (interpr√©tabilit√©)
- Pas de surapprentissage (√©cart R¬≤ train/test minimal)

‚ö†Ô∏è **Limitations** :
- Performance l√©g√®rement inf√©rieure √† Gradient Boosting (-0.4% R¬≤)
- Moins transparent qu'une r√©gression lin√©aire
- Taille du mod√®le plus importante

**üîπ Classification : Random Forest**

‚úÖ **Avantages** :
- Excellentes performances : ROC-AUC = 0.931, F1-Score = 0.605
- Meilleur recall (0.900) ‚Üí d√©tecte 90% des vraies passoires
- Gestion optimale du d√©s√©quilibre de classes (class_weight='balanced')
- Robustesse et stabilit√© sur donn√©es h√©t√©rog√®nes
- Temps d'entra√Ænement acceptable (~54s)

‚ö†Ô∏è **Limitations** :
- Precision mod√©r√©e (0.456) ‚Üí compromis acceptable pour la d√©tection
- Moins interpr√©table qu'une r√©gression logistique
#### **Comparaison des algorithmes test√©s**

| Crit√®re | Ridge | Random Forest | Gradient Boosting | XGBoost |
|---------|-------|---------------|-------------------|---------|
| Performance r√©gression | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Performance classification | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Vitesse entra√Ænement | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Vitesse pr√©diction | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Interpr√©tabilit√© | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Gestion non-lin√©arit√©s | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| R√©gularisation int√©gr√©e | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Gestion d√©s√©quilibre classes | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Conclusion** : 
- **Random Forest** a √©t√© retenu pour les **deux t√¢ches** gr√¢ce √† son excellent compromis performance/rapidit√©
- **R√©gression** : Performances proches de Gradient Boosting avec un temps d'entra√Ænement 6√ó plus rapide
- **Classification** : Meilleur recall (0.900) et ROC-AUC (0.931) pour maximiser la d√©tection des passoires √©nerg√©tiques
- **Uniformit√©** : Un seul algorithme facilite la maintenance et le d√©ploiement de l'application

## 4. Potentiels d'am√©lioration

### 4.1 Optimisation des hyperparam√®tres

**Limitation actuelle** : GridSearchCV avec grilles r√©duites (contraintes de temps/ressources)

**Am√©lioration propos√©e** :
- **Recherche plus exhaustive** : √âlargir les grilles d'hyperparam√®tres test√©s
- **RandomizedSearchCV** : Explorer un espace de param√®tres plus large avec √©chantillonnage al√©atoire
- **Temps d'entra√Ænement** : Accepter des sessions d'entra√Ænement plus longues sur infrastructure d√©di√©e (cloud)

**Impact attendu** : +1-3% de performance, r√©duction du surapprentissage

### 4.2 S√©lection statistique des variables

**Limitation actuelle** : Corr√©lation de Pearson et Chi-2 uniquement

**Am√©lioration propos√©e** :
- **V de Cramer** : Mesure d'association entre variables cat√©gorielles (sym√©trique, normalis√©e [0,1])
- **Mutual Information** : Capturer les relations non-lin√©aires entre features et cible
- **ANOVA F-test** : Alternative au Chi-2 pour variables cat√©gorielles vs num√©riques
- **Recursive Feature Elimination (RFE)** : S√©lection it√©rative guid√©e par le mod√®le

**Impact attendu** : Meilleure identification des features pertinentes, r√©duction du bruit

### 4.3 Exploration d'autres algorithmes

**Limitation actuelle** : 3 mod√®les test√©s (Ridge, Random Forest, Gradient Boosting)

**Am√©lioration propos√©e** :
- **LightGBM** : Alternative rapide et performante pour grands datasets
- **CatBoost** : Excellente gestion native des variables cat√©gorielles
- **R√©seaux de neurones** : MLP (Multi-Layer Perceptron) ou architectures plus complexes
- **Stacking/Ensembling** : Combinaison de plusieurs mod√®les pour am√©liorer les pr√©dictions

**Impact attendu** : +2-5% de performance potentielle, meilleure g√©n√©ralisation

### 4.4 Extension des features utilisateur

**Limitation actuelle** : 10 features simplifi√©es uniquement

**Am√©lioration propos√©e** :
- **Mode "Avanc√©"** : Proposer un formulaire optionnel avec 15-20 variables suppl√©mentaires
- **Variables techniques simplifi√©es** :
    - Type de vitrage (simple/double/triple) ‚Üí accessible visuellement
    - Pr√©sence de VMC ‚Üí information simple oui/non
    - Orientation principale ‚Üí intuitive pour l'utilisateur
    - Ann√©e de r√©novation ‚Üí compl√©mentaire √† la p√©riode de construction
    - Type de chauffage secondaire ‚Üí compl√®te l'information √©nerg√©tique
- **Profils utilisateur** : 
    - Mode "Rapide" (10 features actuelles)
    - Mode "D√©taill√©" (20-25 features)
    - Mode "Expert" (40+ features pour professionnels)

**Impact attendu** : +3-7% de performance pour utilisateurs avanc√©s, segmentation des usages

### 4.5 Am√©liorations m√©thodologiques

**Autres pistes d'optimisation** :
- **Feature Engineering** : Cr√©er des interactions entre variables (ex: `surface √ó isolation`)
- **Validation externe** : Tester sur donn√©es d'autres d√©partements (g√©n√©ralisation g√©ographique)

**Impact attendu** : Robustesse accrue, meilleure transf√©rabilit√© du mod√®le

---

## 5. Conclusion

### 5.1 Synth√®se des r√©sultats

Ce projet a abouti au d√©veloppement de **deux mod√®les de Machine Learning performants et d√©ployables** :

**üéØ R√©gression (Random Forest)**
- R¬≤ = 0.592 ‚Üí Explique 59.2% de la variance
- RMSE = 35 351 kWhep/an
- 9 features seulement ‚Üí UX optimis√©e
- Temps d'entra√Ænement rapide (26s) ‚Üí Facilite le r√©-entra√Ænement

**üéØ Classification (Random Forest)**
- ROC-AUC = 0.931 ‚Üí Excellente discrimination
- Recall = 0.900 ‚Üí D√©tecte 90% des passoires √©nerg√©tiques
- F1-Score = 0.605 ‚Üí Compromis adapt√© √† la d√©tection
- Precision = 0.456 ‚Üí Acceptable pour maximiser la d√©tection

### 5.2 R√©ponse aux contraintes initiales

‚úÖ **Simplicit√©** : R√©duction de 236 ‚Üí 9 variables r√©ussie  
‚úÖ **Performance** : ~75% de r√©tention vs mod√®le complet (36 features)  
‚úÖ **Accessibilit√©** : Informations connues par l'utilisateur + calcul automatique (zone climatique, altitude)  
‚úÖ **Rapidit√©** : ~2 minutes de saisie estim√©es  
‚úÖ **Uniformit√©** : Un seul algorithme (Random Forest) pour les deux t√¢ches ‚Üí maintenance simplifi√©e

### 5.3 Valeur ajout√©e pour la webapp

L'application permettra aux utilisateurs de :
1. **Estimer leur consommation** √©nerg√©tique en quelques clics
2. **Identifier rapidement** si leur logement est une passoire √©nerg√©tique (90% de d√©tection)
3. **Prioriser les travaux** de r√©novation √©nerg√©tique via l'importance des features
4. **Anticiper les co√ªts** √©nerg√©tiques annuels avec une pr√©cision acceptable



