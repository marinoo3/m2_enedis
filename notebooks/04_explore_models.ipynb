{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67412fa4",
   "metadata": {},
   "source": [
    "# Notebook exploratoire — classification des \"passoires énergétiques\"\n",
    "\n",
    "Objectif : guider l'exploration du jeu de données `logements_74.csv` en vue de construire ensuite un modèle de classification qui prédit les \"passoires énergétiques\" (étiquettes DPE faibles) et un modèle de régression qui prédit la consommation énergétique.\n",
    "\n",
    "Note : on définit une *passoire énergétique* si l'`etiquette_dpe` est `F` ou `G` (définition courante en France)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports et paramètres d'affichage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52affc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier (chemin relatif) et vérification rapide\n",
    "df = pd.read_csv(\"logements_74.csv\", low_memory=False)\n",
    "print('Shape :', df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des types et des valeurs manquantes de base\n",
    "display(df.info())\n",
    "# Résumé numérique succinct pour repérer distributions et anomalies\n",
    "display(df.select_dtypes(include=[np.number]).describe().T)\n",
    "\n",
    "# Liste des colonnes disponibles (utile pour repérer noms exacts)\n",
    "print('\\nNombre de colonnes :', len(df.columns))\n",
    "print(df.columns.tolist()[:80])  # affiche les 80 premières colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa05321",
   "metadata": {},
   "source": [
    "### Définition de la cible (et création d'une colonne binaire `passoire`)\n",
    "\n",
    "Nous partons de l'hypothèse suivante : `passoire` = True si `etiquette_dpe` est `F` ou `G`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb268cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardiser l'orthographe et créer la cible\n",
    "df['etiquette_dpe'] = df['etiquette_dpe'].astype(str).str.strip().str.upper().replace({'NAN': np.nan, 'NONE': np.nan})\n",
    "# Définition: F ou G => passoire\n",
    "df['passoire'] = df['etiquette_dpe'].isin(['F','G']).astype(int)\n",
    "print('Distribution de la cible (passoire=1):')\n",
    "display(df['passoire'].value_counts(dropna=False))\n",
    "display(df['etiquette_dpe'].value_counts(dropna=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d05d2",
   "metadata": {},
   "source": [
    "### Valeurs manquantes — colonne par colonne (pour prioriser le nettoyage)\n",
    "\n",
    "On calcule le pourcentage de valeurs manquantes et on affiche les colonnes les plus affectées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isna().mean().sort_values(ascending=False)\n",
    "display(missing.head(60))\n",
    "# Colonnes avec moins de 5% de manquants — candidates faciles pour modélisation rapide\n",
    "display(missing[missing < 0.05].sort_values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c277b",
   "metadata": {},
   "source": [
    "### Vérification rapide des coordonnées géographiques (si présentes)\n",
    "\n",
    "Si `coordonnee_cartographique_x_ban` et `coordonnee_cartographique_y_ban` existent et sont valides, on peut vérifier une dispersion sommaire pour détecter anomalies (ex : 0/NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_col = 'coordonnee_cartographique_y_ban'\n",
    "lon_col = 'coordonnee_cartographique_x_ban'\n",
    "if lat_col in df.columns and lon_col in df.columns:\n",
    "    sub = df[[lon_col, lat_col, 'passoire']].dropna().sample(min(2000, len(df.dropna(subset=[lon_col, lat_col]))))\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.scatterplot(data=sub, x=lon_col, y=lat_col, hue='passoire', s=10, alpha=0.7)\n",
    "    plt.title('Répartition géographique (échantillon)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Colonnes de coordonnées non trouvées ou manquantes dans le dataset.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['etiquette_dpe'].value_counts().plot(kind='bar', figsize=(8,5), title='Distribution des étiquettes DPE')\n",
    "plt.xlabel('Étiquette DPE') \n",
    "plt.ylabel('Nombre de logements')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05449c",
   "metadata": {},
   "source": [
    "## Partie 1 : Préparation des données et identification des variables cibles\n",
    "\n",
    "### 1.1 Identification de la variable de consommation électrique\n",
    "\n",
    "Nous allons d'abord identifier les colonnes liées à la consommation électrique pour notre modèle de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e60f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche des colonnes liées à la consommation\n",
    "conso_cols = [col for col in df.columns if 'conso' in col.lower()]\n",
    "print('Colonnes de consommation disponibles:')\n",
    "for col in conso_cols:\n",
    "    print(f\"  - {col}\")\n",
    "    \n",
    "# Afficher statistiques pour les colonnes de consommation numériques\n",
    "print('\\n--- Statistiques des colonnes de consommation ---')\n",
    "for col in conso_cols:\n",
    "    if df[col].dtype in ['float64', 'int64']:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Valeurs manquantes: {df[col].isna().sum()} ({df[col].isna().mean()*100:.2f}%)\")\n",
    "        print(f\"  Min: {df[col].min():.2f}, Max: {df[col].max():.2f}, Moyenne: {df[col].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccb026",
   "metadata": {},
   "source": [
    "### 1.2 Identification des variables explicatives pertinentes\n",
    "\n",
    "Nous allons identifier les variables potentiellement importantes pour nos modèles :\n",
    "- Variables géographiques (zone climatique, altitude, coordonnées)\n",
    "- Caractéristiques du logement (surface, année de construction, isolation, etc.)\n",
    "- Caractéristiques énergétiques (type de chauffage, isolation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche de colonnes clés pour l'analyse\n",
    "keywords = {\n",
    "    'géographique': ['altitude', 'zone', 'climat', 'region', 'coordonnee'],\n",
    "    'logement': ['surface', 'annee', 'construction', 'niveau', 'hauteur'],\n",
    "    'isolation': ['isolation', 'menuiserie', 'mur', 'toiture', 'plancher', 'enveloppe'],\n",
    "    'chauffage': ['chauffage', 'generateur', 'energie', 'emetteur'],\n",
    "    'ecs': ['ecs', 'eau_chaude'],\n",
    "    'dpe': ['dpe', 'etiquette', 'classe', 'emission', 'ges']\n",
    "}\n",
    "\n",
    "print(\"=== Variables disponibles par catégorie ===\\n\")\n",
    "variables_interessantes = []\n",
    "for categorie, mots_cles in keywords.items():\n",
    "    cols = []\n",
    "    for mot in mots_cles:\n",
    "        cols.extend([c for c in df.columns if mot.lower() in c.lower() and c not in cols])\n",
    "    print(f\"\\n{categorie.upper()} ({len(cols)} colonnes):\")\n",
    "    for col in cols[:15]:  # Limite à 15 pour lisibilité\n",
    "        variables_interessantes.append(col)\n",
    "        taux_manquant = df[col].isna().mean() * 100\n",
    "        print(f\"  - {col} (manquants: {taux_manquant:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea22d66",
   "metadata": {},
   "source": [
    "## Partie 2 : Prétraitement et sélection des features\n",
    "\n",
    "### 2.1 Préparation du dataset pour la modélisation\n",
    "\n",
    "Nous allons :\n",
    "1. Séparer les variables numériques et catégorielles\n",
    "2. Gérer les valeurs manquantes\n",
    "3. Créer un ensemble de features potentielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports supplémentaires pour la modélisation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif, mutual_info_regression, mutual_info_classif, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la cible pour la régression (consommation électrique)\n",
    "# Choisissons une variable de consommation appropriée\n",
    "target_regression = 'conso_5_usages_ep'  # Consommation 5 usages en énergie primaire\n",
    "\n",
    "# Vérifier la disponibilité de la cible\n",
    "print(f\"Target régression : {target_regression}\")\n",
    "print(f\"Valeurs manquantes : {df[target_regression].isna().sum()} ({df[target_regression].isna().mean()*100:.2f}%)\")\n",
    "print(f\"Statistiques :\\n{df[target_regression].describe()}\")\n",
    "\n",
    "# Créer un dataframe de travail sans les lignes où la cible est manquante\n",
    "df_work = df[df[target_regression].notna() & df['passoire'].notna()].copy()\n",
    "print(f\"\\nDataset de travail : {df_work.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d012068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes potentiellement utiles\n",
    "# On exclut les colonnes liées à la cible (consommation, émission, DPE, coûts)\n",
    "# ET les calculs intermédiaires du DPE (besoins, déperditions) qui sont des fuites\n",
    "colonnes_a_exclure = ['conso', 'emission', 'cout', 'etiquette_dpe', 'etiquette_ges',\n",
    "                      'date_', 'passoire', '_rand', '_geopoint',\n",
    "                      'besoin_', 'deperdition',  # Calculs intermédiaires DPE\n",
    "                      'methode_application_dpe', 'modele_dpe',  # Variables administratives/techniques DPE\n",
    "                      'surface_chauffee_installation_chauffage_n1',  # VIF > 10 (multicolinéarité)\n",
    "                      'surface_habitable_desservie_par_installation_ecs_n1']  # VIF > 10 (multicolinéarité)\n",
    "\n",
    "# Créer liste des features candidates\n",
    "features_candidates = []\n",
    "for col in df_work.columns:\n",
    "    # Exclure les colonnes liées aux résultats\n",
    "    if not any(exclu in col.lower() for exclu in colonnes_a_exclure):\n",
    "        features_candidates.append(col)\n",
    "\n",
    "print(f\"Nombre de features candidates : {len(features_candidates)}\")\n",
    "print(f\"\\nPremières features : {features_candidates[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer variables numériques et catégorielles\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for col in features_candidates:\n",
    "    if df_work[col].dtype in ['float64', 'int64']:\n",
    "        # Vérifier si pas trop de valeurs manquantes (< 50%)\n",
    "        if df_work[col].isna().mean() < 0.5:\n",
    "            numeric_features.append(col)\n",
    "    elif df_work[col].dtype == 'object':\n",
    "        # Pour les catégorielles, garder celles avec peu de modalités et peu de manquants\n",
    "        n_unique = df_work[col].nunique()\n",
    "        if n_unique < 50 and df_work[col].isna().mean() < 0.5:\n",
    "            categorical_features.append(col)\n",
    "\n",
    "print(f\"Variables numériques sélectionnées : {len(numeric_features)}\")\n",
    "print(f\"Variables catégorielles sélectionnées : {len(categorical_features)}\")\n",
    "print(f\"\\nExemples de variables numériques : {numeric_features[:10]}\")\n",
    "print(f\"\\nExemples de variables catégorielles : {categorical_features[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1143737",
   "metadata": {},
   "source": [
    "### Analyse approfondie de la sélection des variables\n",
    "\n",
    "Avant de procéder à la sélection automatique, nous allons analyser en détail la pertinence des variables selon les meilleures pratiques :\n",
    "\n",
    "1. **Vérification des variables clés recommandées** (structure, isolation, chauffage, ECS, climat)\n",
    "2. **Analyse des corrélations** avec la cible (régression)\n",
    "3. **Tests statistiques Chi-2** pour les variables catégorielles (classification)\n",
    "4. **Détection de la multicolinéarité** (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791cd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1. VÉRIFICATION DES VARIABLES CLÉS RECOMMANDÉES\n",
    "# ========================================\n",
    "\n",
    "variables_cles = {\n",
    "    'structure': ['surface_habitable', 'nombre_niveau', 'hauteur_sous_plafond', \n",
    "                  'periode_construction', 'annee_construction', 'volume_stockage', 'type_batiment',\n",
    "                  'numero_etage', 'logement_traversant'],\n",
    "    'isolation': ['qualite_isolation_enveloppe', 'qualite_isolation_murs', \n",
    "                  'qualite_isolation_menuiseries', 'qualite_isolation_plancher_bas',\n",
    "                  'qualite_isolation_plancher_haut', 'ubat', 'deperdition'],\n",
    "    'chauffage': ['type_generateur_chauffage', 'type_energie_principale_chauffage',\n",
    "                  'type_emetteur', 'besoin_chauffage', 'type_installation_chauffage'],\n",
    "    'ecs': ['type_generateur_n1_ecs', 'type_energie_principale_ecs', 'besoin_ecs'],\n",
    "    'climat': ['zone_climatique', 'classe_altitude', 'altitude_moyenne', 'altitude'],\n",
    "    'ventilation': ['type_ventilation', 'classe_inertie', 'indicateur_confort']\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VARIABLES CLÉS RECOMMANDÉES - PRÉSENCE ET QUALITÉ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "variables_trouvees = {}\n",
    "\n",
    "for categorie, vars_list in variables_cles.items():\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"{categorie.upper()}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    variables_trouvees[categorie] = []\n",
    "    \n",
    "    for var_pattern in vars_list:\n",
    "        matching = [c for c in df_work.columns if var_pattern.lower() in c.lower() \n",
    "                   and not any(exclu in c.lower() for exclu in ['conso', 'emission', 'cout', 'etiquette'])]\n",
    "        \n",
    "        if matching:\n",
    "            for var in matching:\n",
    "                missing_pct = df_work[var].isna().mean() * 100\n",
    "                dtype = df_work[var].dtype\n",
    "                n_unique = df_work[var].nunique() if dtype == 'object' else '-'\n",
    "                \n",
    "                status = \"✓\" if missing_pct < 50 else \"⚠\"\n",
    "                print(f\"  {status} {var:50s} | Manquants: {missing_pct:5.1f}% | Type: {str(dtype):10s} | Modalités: {n_unique}\")\n",
    "                \n",
    "                if missing_pct < 50:  # Garder seulement si < 50% de manquants\n",
    "                    variables_trouvees[categorie].append(var)\n",
    "        else:\n",
    "            print(f\"  ✗ Pattern '{var_pattern}' non trouvé\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RÉSUMÉ: {sum(len(v) for v in variables_trouvees.values())} variables clés disponibles\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2. CORRÉLATIONS AVEC LA CIBLE (RÉGRESSION)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP CORRÉLATIONS AVEC LA CONSOMMATION ÉLECTRIQUE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identifier les colonnes numériques (excluant les fuites de données)\n",
    "numeric_cols_for_corr = [c for c in df_work.select_dtypes(include=[np.number]).columns \n",
    "                         if not any(exclu in c.lower() for exclu in ['conso', 'emission', 'cout', 'etiquette_dpe', 'etiquette_ges',\n",
    "                                                                       'passoire', 'besoin_', 'deperdition',\n",
    "                                                                       'methode_application_dpe', 'modele_dpe',\n",
    "                                                                       'surface_chauffee_installation_chauffage_n1',\n",
    "                                                                       'surface_habitable_desservie_par_installation_ecs_n1'])]  # Exclusion calculs DPE + variables admin + VIF > 10\n",
    "\n",
    "# Calculer les corrélations avec la cible\n",
    "correlations = df_work[numeric_cols_for_corr].corrwith(df_work[target_regression]).abs().sort_values(ascending=False)\n",
    "\n",
    "# Afficher les top 30\n",
    "print(\"\\nTop 30 corrélations (valeur absolue) :\")\n",
    "print(\"─\"*80)\n",
    "for i, (var, corr) in enumerate(correlations.head(30).items(), 1):\n",
    "    corr_signed = df_work[var].corr(df_work[target_regression])\n",
    "    direction = \"↑ Positive\" if corr_signed > 0 else \"↓ Négative\"\n",
    "    print(f\"{i:2d}. {var:50s} | r = {corr_signed:+.4f} ({direction})\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_corr = correlations.head(25)\n",
    "colors = ['green' if df_work[var].corr(df_work[target_regression]) > 0 else 'red' \n",
    "          for var in top_corr.index]\n",
    "sns.barplot(x=top_corr.values, y=top_corr.index, palette=colors)\n",
    "plt.title('Top 25 Corrélations avec la Consommation Électrique\\n(Vert = corrélation positive, Rouge = corrélation négative)')\n",
    "plt.xlabel('Corrélation absolue')\n",
    "plt.axvline(x=0.3, color='orange', linestyle='--', label='Seuil modéré (0.3)')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Seuil fort (0.5)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Variables avec corrélation > 0.3: {(correlations > 0.3).sum()}\")\n",
    "print(f\"✓ Variables avec corrélation > 0.5: {(correlations > 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. TEST CHI-2 POUR VARIABLES CATÉGORIELLES (CLASSIFICATION)\n",
    "# ========================================\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST CHI-2 POUR LA PRÉDICTION DES PASSOIRES ÉNERGÉTIQUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Encoder les variables catégorielles pour le test Chi-2\n",
    "categorical_for_chi2 = [col for col in categorical_features if col in df_work.columns]\n",
    "\n",
    "if len(categorical_for_chi2) > 0:\n",
    "    print(f\"\\nAnalyse de {len(categorical_for_chi2)} variables catégorielles...\\n\")\n",
    "    \n",
    "    # Encoder les catégorielles (factorize convertit en entiers)\n",
    "    categorical_encoded = pd.DataFrame()\n",
    "    for col in categorical_for_chi2:\n",
    "        categorical_encoded[col] = pd.factorize(df_work[col].fillna('MISSING'))[0]\n",
    "    \n",
    "    # Test Chi-2\n",
    "    chi2_scores, p_values = chi2(categorical_encoded, df_work['passoire'])\n",
    "    \n",
    "    # Créer un DataFrame des résultats\n",
    "    chi2_df = pd.DataFrame({\n",
    "        'Variable': categorical_for_chi2,\n",
    "        'Chi2_Score': chi2_scores,\n",
    "        'P_Value': p_values\n",
    "    }).sort_values('Chi2_Score', ascending=False)\n",
    "    \n",
    "    # Afficher les top 20\n",
    "    print(\"Top 20 variables catégorielles les plus discriminantes :\")\n",
    "    print(\"─\"*80)\n",
    "    for i, row in chi2_df.head(20).iterrows():\n",
    "        significance = \"***\" if row['P_Value'] < 0.001 else \"**\" if row['P_Value'] < 0.01 else \"*\" if row['P_Value'] < 0.05 else \"\"\n",
    "        print(f\"{row['Variable']:50s} | Chi2 = {row['Chi2_Score']:10.2f} | p-value = {row['P_Value']:.2e} {significance}\")\n",
    "    \n",
    "    print(\"\\n*** p < 0.001 (très significatif), ** p < 0.01, * p < 0.05\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_chi2 = chi2_df.head(25)\n",
    "    sns.barplot(data=top_chi2, y='Variable', x='Chi2_Score', palette='plasma')\n",
    "    plt.title('Top 25 Variables Catégorielles - Score Chi-2 pour Prédiction Passoires')\n",
    "    plt.xlabel('Score Chi-2 (plus élevé = plus discriminant)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Variables significatives (p < 0.05): {(chi2_df['P_Value'] < 0.05).sum()}\")\n",
    "    print(f\"✓ Variables très significatives (p < 0.001): {(chi2_df['P_Value'] < 0.001).sum()}\")\n",
    "else:\n",
    "    print(\"⚠ Aucune variable catégorielle disponible pour le test Chi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28081f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4. DÉTECTION DE LA MULTICOLINÉARITÉ (VIF)\n",
    "# ========================================\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DÉTECTION DE LA MULTICOLINÉARITÉ (VIF - Variance Inflation Factor)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCritères d'interprétation :\")\n",
    "print(\"  • VIF < 5   : Pas de multicolinéarité\")\n",
    "print(\"  • VIF 5-10  : Multicolinéarité modérée\")\n",
    "print(\"  • VIF > 10  : Multicolinéarité forte (variable redondante)\\n\")\n",
    "\n",
    "# Prendre un échantillon pour accélérer le calcul\n",
    "sample_size = min(5000, len(df_work))\n",
    "print(f\"Calcul sur un échantillon de {sample_size} observations...\")\n",
    "\n",
    "# Sélectionner les variables numériques pertinentes\n",
    "numeric_for_vif = [col for col in numeric_features if col in df_work.columns][:50]  # Limiter à 50 pour la rapidité\n",
    "\n",
    "if len(numeric_for_vif) > 2:\n",
    "    # Échantillonner et imputer\n",
    "    X_vif = df_work[numeric_for_vif].sample(n=sample_size, random_state=42)\n",
    "    X_vif_filled = X_vif.fillna(X_vif.median())\n",
    "    \n",
    "    # Retirer les colonnes avec variance nulle\n",
    "    X_vif_filled = X_vif_filled.loc[:, X_vif_filled.std() > 0]\n",
    "    \n",
    "    print(f\"Analyse de {len(X_vif_filled.columns)} variables numériques...\\n\")\n",
    "    \n",
    "    # Calculer les VIF\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X_vif_filled.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_vif_filled.values, i) \n",
    "                       for i in range(len(X_vif_filled.columns))]\n",
    "    vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "    \n",
    "    # Afficher les variables avec forte multicolinéarité\n",
    "    print(\"Variables avec FORTE multicolinéarité (VIF > 10) :\")\n",
    "    print(\"─\"*80)\n",
    "    high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "    if len(high_vif) > 0:\n",
    "        for i, row in high_vif.iterrows():\n",
    "            print(f\"⚠ {row['Variable']:50s} | VIF = {row['VIF']:8.2f}\")\n",
    "        print(f\"\\n⚠ {len(high_vif)} variables avec forte multicolinéarité détectées\")\n",
    "        print(\"→ Considérer la suppression de ces variables ou utiliser PCA/régularisation\")\n",
    "    else:\n",
    "        print(\"✓ Aucune variable avec VIF > 10\")\n",
    "    \n",
    "    # Afficher les variables avec multicolinéarité modérée\n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"Variables avec multicolinéarité MODÉRÉE (5 < VIF < 10) :\")\n",
    "    print(\"─\"*80)\n",
    "    moderate_vif = vif_data[(vif_data['VIF'] > 5) & (vif_data['VIF'] <= 10)]\n",
    "    if len(moderate_vif) > 0:\n",
    "        for i, row in moderate_vif.head(15).iterrows():\n",
    "            print(f\"  {row['Variable']:50s} | VIF = {row['VIF']:8.2f}\")\n",
    "        print(f\"\\n{len(moderate_vif)} variables avec multicolinéarité modérée\")\n",
    "    else:\n",
    "        print(\"✓ Aucune variable avec VIF entre 5 et 10\")\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_vif = vif_data.head(30)\n",
    "    colors = ['red' if x > 10 else 'orange' if x > 5 else 'green' for x in top_vif['VIF']]\n",
    "    sns.barplot(data=top_vif, y='Variable', x='VIF', palette=colors)\n",
    "    plt.axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='Seuil modéré (5)')\n",
    "    plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='Seuil fort (10)')\n",
    "    plt.title('Top 30 Variables - VIF (Variance Inflation Factor)')\n",
    "    plt.xlabel('VIF')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommandations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMANDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    if len(high_vif) > 0:\n",
    "        print(f\"⚠ {len(high_vif)} variables redondantes détectées (VIF > 10)\")\n",
    "        print(\"  → Options: supprimer, utiliser PCA, ou régularisation (Lasso, Ridge)\")\n",
    "    else:\n",
    "        print(\"✓ Pas de problème majeur de multicolinéarité\")\n",
    "else:\n",
    "    print(\"⚠ Pas assez de variables numériques pour le calcul du VIF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. ANALYSE SPÉCIFIQUE : ALTITUDE ET ZONE CLIMATIQUE\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE SPÉCIFIQUE SAVOIE (74) - IMPACT ALTITUDE ET CLIMAT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rechercher les colonnes liées à l'altitude et au climat\n",
    "altitude_cols = [col for col in df_work.columns if 'altitude' in col.lower()]\n",
    "climat_cols = [col for col in df_work.columns if 'climat' in col.lower() or 'zone' in col.lower()]\n",
    "\n",
    "print(\"\\n1. Variables d'ALTITUDE disponibles :\")\n",
    "print(\"─\"*80)\n",
    "for col in altitude_cols:\n",
    "    if col in numeric_cols_for_corr:\n",
    "        corr_conso = df_work[col].corr(df_work[target_regression])\n",
    "        missing = df_work[col].isna().mean() * 100\n",
    "        print(f\"  {col:40s} | Corrélation: {corr_conso:+.4f} | Manquants: {missing:.1f}%\")\n",
    "        \n",
    "        # Statistiques descriptives\n",
    "        print(f\"    → Min: {df_work[col].min():.0f}m, Max: {df_work[col].max():.0f}m, Moyenne: {df_work[col].mean():.0f}m\")\n",
    "\n",
    "print(\"\\n2. Variables de ZONE CLIMATIQUE disponibles :\")\n",
    "print(\"─\"*80)\n",
    "for col in climat_cols:\n",
    "    missing = df_work[col].isna().mean() * 100\n",
    "    n_unique = df_work[col].nunique()\n",
    "    print(f\"  {col:40s} | Modalités: {n_unique:3d} | Manquants: {missing:.1f}%\")\n",
    "\n",
    "# Analyse de la distribution de l'altitude si disponible\n",
    "if altitude_cols:\n",
    "    # Identifier la colonne numérique d'altitude (pas la classe catégorielle)\n",
    "    numeric_altitude_col = None\n",
    "    for col in altitude_cols:\n",
    "        if df_work[col].dtype in ['float64', 'int64']:\n",
    "            numeric_altitude_col = col\n",
    "            break\n",
    "    \n",
    "    if numeric_altitude_col is None:\n",
    "        print(\"⚠️ Aucune colonne d'altitude numérique trouvée\")\n",
    "        print(f\"Colonnes disponibles: {altitude_cols}\")\n",
    "    else:\n",
    "        main_altitude_col = numeric_altitude_col\n",
    "        print(f\"\\n✓ Colonne d'altitude utilisée: {main_altitude_col}\")\n",
    "        \n",
    "        # S'assurer que l'altitude est numérique\n",
    "        altitude_data = pd.to_numeric(df_work[main_altitude_col], errors='coerce')\n",
    "        \n",
    "        # Créer un DataFrame propre pour l'analyse\n",
    "        df_altitude_clean = pd.DataFrame({\n",
    "            'altitude': altitude_data,\n",
    "            'conso': df_work[target_regression]\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(df_altitude_clean) > 0:\n",
    "            print(f\"✓ {len(df_altitude_clean)} observations valides pour l'analyse graphique\")\n",
    "            \n",
    "            # Distribution de l'altitude\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "            \n",
    "            # Subplot 1: Histogramme\n",
    "            axes[0].hist(df_altitude_clean['altitude'], bins=50, edgecolor='black')\n",
    "            axes[0].set_title(f'Distribution de {main_altitude_col}')\n",
    "            axes[0].set_xlabel('Altitude (m)')\n",
    "            axes[0].set_ylabel('Nombre de logements')\n",
    "            \n",
    "            # Subplot 2: Boxplot par tranche d'altitude\n",
    "            try:\n",
    "                df_altitude_clean['altitude_bin'] = pd.cut(df_altitude_clean['altitude'], bins=5)\n",
    "                sns.boxplot(data=df_altitude_clean, x='altitude_bin', y='conso', ax=axes[1])\n",
    "                axes[1].set_title('Consommation par tranche d\\'altitude')\n",
    "                axes[1].set_xlabel('Altitude (m)')\n",
    "                axes[1].set_ylabel('Consommation (kWh/m²/an)')\n",
    "                axes[1].tick_params(axis='x', rotation=45)\n",
    "            except Exception as e:\n",
    "                axes[1].text(0.5, 0.5, f'Erreur boxplot:\\n{str(e)}', \n",
    "                            ha='center', va='center', transform=axes[1].transAxes)\n",
    "                axes[1].set_title('Consommation par tranche d\\'altitude (erreur)')\n",
    "            \n",
    "            # Subplot 3: Scatter plot\n",
    "            axes[2].scatter(df_altitude_clean['altitude'], df_altitude_clean['conso'], alpha=0.3, s=1)\n",
    "            axes[2].set_xlabel('Altitude (m)')\n",
    "            axes[2].set_ylabel('Consommation (kWh/m²/an)')\n",
    "            correlation = df_altitude_clean['altitude'].corr(df_altitude_clean['conso'])\n",
    "            axes[2].set_title(f'Corrélation: {correlation:.4f}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Analyse par tranches d'altitude\n",
    "            print(\"\\n3. Consommation moyenne par TRANCHE D'ALTITUDE :\")\n",
    "            print(\"─\"*80)\n",
    "            \n",
    "            # Créer les tranches avec les données propres\n",
    "            df_altitude_clean['altitude_tranches'] = pd.cut(\n",
    "                df_altitude_clean['altitude'], \n",
    "                bins=[0, 400, 600, 800, 1000, 3000], \n",
    "                labels=['< 400m', '400-600m', '600-800m', '800-1000m', '> 1000m']\n",
    "            )\n",
    "            \n",
    "            # Joindre avec la colonne passoire\n",
    "            df_for_groupby = df_altitude_clean.copy()\n",
    "            df_for_groupby['passoire'] = df_work.loc[df_altitude_clean.index, 'passoire']\n",
    "            \n",
    "            altitude_analysis = df_for_groupby.groupby('altitude_tranches', observed=True).agg({\n",
    "                'conso': ['mean', 'median', 'count'],\n",
    "                'passoire': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            altitude_analysis.columns = ['Conso_Moyenne', 'Conso_Médiane', 'Nb_Logements', 'Taux_Passoires']\n",
    "            print(altitude_analysis)\n",
    "            \n",
    "            print(\"\\n→ En Haute-Savoie, l'altitude est un facteur MAJEUR de consommation énergétique\")\n",
    "            print(\"→ Plus l'altitude augmente, plus les besoins en chauffage sont importants\")\n",
    "        else:\n",
    "            print(\"⚠️ Aucune donnée valide pour créer les graphiques d'altitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b773bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. SÉLECTION INTELLIGENTE DES FEATURES BASÉE SUR LES ANALYSES\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SÉLECTION INTELLIGENTE DES FEATURES (Basée sur analyses statistiques)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# ÉTAPE 1 : Identifier les features à EXCLURE (VIF > 10)\n",
    "# ─────────────────────────────────────────\n",
    "features_to_exclude_vif = []\n",
    "if 'vif_data' in locals() and len(vif_data) > 0:\n",
    "    high_vif_vars = vif_data[vif_data['VIF'] > 10]['Variable'].tolist()\n",
    "    features_to_exclude_vif.extend(high_vif_vars)\n",
    "    print(f\"\\n⚠ Variables exclues (VIF > 10): {len(features_to_exclude_vif)}\")\n",
    "    for var in features_to_exclude_vif[:10]:\n",
    "        print(f\"  - {var}\")\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# ÉTAPE 2 : Sélectionner les MEILLEURES features NUMÉRIQUES (corrélation)\n",
    "# ─────────────────────────────────────────\n",
    "selected_numeric_smart = []\n",
    "if 'correlations' in locals():\n",
    "    # SEUIL AJUSTÉ : 0.10 pour capturer plus de variables pertinentes\n",
    "    # (0.15 était trop restrictif et ne donnait que 6 variables)\n",
    "    CORRELATION_THRESHOLD = 0.10\n",
    "    \n",
    "    # Prendre les variables avec corrélation > seuil\n",
    "    strong_corr_vars = correlations[correlations > CORRELATION_THRESHOLD].index.tolist()\n",
    "    \n",
    "    # Exclure celles avec VIF trop élevé\n",
    "    selected_numeric_smart = [v for v in strong_corr_vars \n",
    "                              if v not in features_to_exclude_vif \n",
    "                              and v in numeric_features]\n",
    "    \n",
    "    print(f\"\\n✓ Variables numériques sélectionnées (corrélation > {CORRELATION_THRESHOLD}, VIF OK): {len(selected_numeric_smart)}\")\n",
    "    \n",
    "    # Afficher les détails pour transparence\n",
    "    print(f\"\\n  Détail de la sélection:\")\n",
    "    for i, var in enumerate(selected_numeric_smart, 1):\n",
    "        corr_val = df_work[var].corr(df_work[target_regression])\n",
    "        print(f\"    {i:2d}. {var:50s} | r = {corr_val:+.4f}\")\n",
    "    \n",
    "    # Afficher les variables exclues pour transparence\n",
    "    excluded_corr_vars = [v for v in strong_corr_vars if v not in selected_numeric_smart]\n",
    "    if excluded_corr_vars:\n",
    "        print(f\"\\n  ⚠ Variables avec bonne corrélation mais EXCLUES ({len(excluded_corr_vars)}):\")\n",
    "        for var in excluded_corr_vars[:10]:\n",
    "            corr_val = df_work[var].corr(df_work[target_regression])\n",
    "            if var in features_to_exclude_vif:\n",
    "                reason = \"VIF > 10\"\n",
    "            else:\n",
    "                reason = \"Filtrage initial (manquants ou hors numeric_features)\"\n",
    "            print(f\"      • {var:45s} | r = {corr_val:+.4f} | Raison: {reason}\")\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# ÉTAPE 3 : Sélectionner les MEILLEURES features CATÉGORIELLES (Chi-2)\n",
    "# ─────────────────────────────────────────\n",
    "selected_categorical_smart = []\n",
    "if 'chi2_df' in locals() and len(chi2_df) > 0:\n",
    "    # Prendre les variables significatives (p < 0.05) avec Chi2 élevé\n",
    "    significant_chi2 = chi2_df[chi2_df['P_Value'] < 0.05].sort_values('Chi2_Score', ascending=False)\n",
    "    \n",
    "    # Prendre le top 30 (pour limiter les dimensions après one-hot encoding)\n",
    "    selected_categorical_smart = significant_chi2.head(30)['Variable'].tolist()\n",
    "    selected_categorical_smart = [v for v in selected_categorical_smart if v in categorical_features]\n",
    "    \n",
    "    print(f\"\\n✓ Variables catégorielles sélectionnées (p < 0.05, top 30 Chi-2): {len(selected_categorical_smart)}\")\n",
    "    \n",
    "    # Afficher les détails\n",
    "    print(f\"\\n  Top 15 variables catégorielles:\")\n",
    "    for i, row in significant_chi2.head(15).iterrows():\n",
    "        if row['Variable'] in selected_categorical_smart:\n",
    "            status = \"✓\"\n",
    "        else:\n",
    "            status = \"✗\"\n",
    "        print(f\"    {status} {row['Variable']:50s} | Chi2 = {row['Chi2_Score']:10.2f} | p = {row['P_Value']:.2e}\")\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# ÉTAPE 4 : Combiner et créer la liste finale\n",
    "# ─────────────────────────────────────────\n",
    "selected_features_smart = selected_numeric_smart + selected_categorical_smart\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RÉSUMÉ DE LA SÉLECTION INTELLIGENTE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  • Variables numériques   : {len(selected_numeric_smart):3d}\")\n",
    "print(f\"  • Variables catégorielles: {len(selected_categorical_smart):3d}\")\n",
    "print(f\"  • TOTAL                  : {len(selected_features_smart):3d}\")\n",
    "print(f\"\\n→ Ces features seront utilisées pour l'entraînement des modèles\")\n",
    "\n",
    "# Sauvegarder pour utilisation ultérieure\n",
    "selected_features_info = {\n",
    "    'all': selected_features_smart,\n",
    "    'numeric': selected_numeric_smart,\n",
    "    'categorical': selected_categorical_smart,\n",
    "    'excluded_vif': features_to_exclude_vif\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Sélection intelligente terminée!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fddfb5",
   "metadata": {},
   "source": [
    "#### Visualisation du processus de sélection\n",
    "\n",
    "**Pipeline de sélection des features** :\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ ÉTAPE 1 : Filtrage de base                                     │\n",
    "│ • < 50% valeurs manquantes                                      │\n",
    "│ • < 50 modalités (catégorielles)                                │\n",
    "│ • Exclusion : conso, emission, etiquette_dpe, besoin_, etc.    │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ ÉTAPE 2 : Analyses statistiques                                │\n",
    "│ • Corrélation Pearson (numériques)                             │\n",
    "│ • Test Chi-2 (catégorielles)                                    │\n",
    "│ • VIF - Détection multicolinéarité                             │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ ÉTAPE 3 : Sélection intelligente                               │\n",
    "│ • Variables numériques : corrélation > 0.15 ET VIF < 10        │\n",
    "│ • Variables catégorielles : p-value < 0.05, top 30 Chi-2       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "                            ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│ ÉTAPE 4 : Entraînement des modèles                             │\n",
    "│ • Random Forest (validation de la sélection)                   │\n",
    "│ • Ridge, Gradient Boosting                                      │\n",
    "│ • Utilisation DIRECTE des features sélectionnées                │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PRÉPARATION DES PIPELINES AVEC FEATURES INTELLIGEMMENT SÉLECTIONNÉES\n",
    "# ========================================\n",
    "\n",
    "# Vérifier si la sélection intelligente a été effectuée\n",
    "if 'selected_features_smart' in locals() and len(selected_features_smart) > 0:\n",
    "    print(\"✓ Utilisation des features sélectionnées par analyse statistique\")\n",
    "    features_to_use_numeric = selected_numeric_smart\n",
    "    features_to_use_categorical = selected_categorical_smart\n",
    "else:\n",
    "    print(\"⚠ Sélection intelligente non disponible, utilisation de toutes les features\")\n",
    "    features_to_use_numeric = numeric_features\n",
    "    features_to_use_categorical = categorical_features\n",
    "\n",
    "# Pipeline de prétraitement pour les données numériques\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline de prétraitement pour les données catégorielles\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combinaison des deux pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, features_to_use_numeric),\n",
    "        ('cat', categorical_transformer, features_to_use_categorical)\n",
    "    ])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Pipeline de prétraitement créé avec succès!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  - {len(features_to_use_numeric)} variables numériques (corrélation validée)\")\n",
    "print(f\"  - {len(features_to_use_categorical)} variables catégorielles (Chi-2 validé)\")\n",
    "print(f\"  - TOTAL: {len(features_to_use_numeric) + len(features_to_use_categorical)} variables\")\n",
    "print(\"\\n→ Ces variables ont été sélectionnées sur la base des analyses statistiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30865dd",
   "metadata": {},
   "source": [
    "## Partie 3 : Modèle de Régression - Prédiction de la consommation électrique\n",
    "\n",
    "### 3.1 Préparation des données et division Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données pour la régression avec features intelligemment sélectionnées\n",
    "if 'selected_features_smart' in locals() and len(selected_features_smart) > 0:\n",
    "    X = df_work[selected_features_smart]\n",
    "    print(\"✓ Utilisation des features sélectionnées par analyse statistique\")\n",
    "else:\n",
    "    X = df_work[numeric_features + categorical_features]\n",
    "    print(\"⚠ Utilisation de toutes les features (sélection intelligente non disponible)\")\n",
    "\n",
    "y_regression = df_work[target_regression]\n",
    "\n",
    "# Division train/test (80/20)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_regression, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTaille du jeu d'entraînement : {X_train_reg.shape}\")\n",
    "print(f\"Taille du jeu de test : {X_test_reg.shape}\")\n",
    "print(f\"Nombre de features utilisées : {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d24ea9",
   "metadata": {},
   "source": [
    "### 3.2 Optimisation des hyperparamètres avec GridSearchCV\n",
    "\n",
    "Les features ont été sélectionnées par analyse statistique. Nous allons maintenant optimiser les hyperparamètres des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ENTRAÎNEMENT AVEC LES FEATURES SÉLECTIONNÉES PAR ANALYSE STATISTIQUE\n",
    "# ========================================\n",
    "# Les features utilisées proviennent des analyses de corrélation, Chi-2 et VIF\n",
    "# Pas de réduction supplémentaire, on utilise directement les features validées\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRÉPARATION DES DONNÉES POUR L'ENTRAÎNEMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Features numériques utilisées: {len(features_to_use_numeric)}\")\n",
    "print(f\"✓ Features catégorielles utilisées: {len(features_to_use_categorical)}\")\n",
    "print(f\"✓ TOTAL: {len(features_to_use_numeric) + len(features_to_use_categorical)}\")\n",
    "print(\"\\n→ Ces features ont été sélectionnées sur la base des analyses statistiques\")\n",
    "print(\"→ Aucune réduction supplémentaire n'est effectuée\\n\")\n",
    "\n",
    "# Les données sont déjà préparées (X_train_reg, X_test_reg)\n",
    "# Le preprocessor est déjà configuré avec les bonnes features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f56de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le preprocessor a déjà été créé avec les features sélectionnées intelligemment\n",
    "# Les données X_train_reg et X_test_reg contiennent déjà les bonnes features\n",
    "\n",
    "print(\"✓ Données prêtes pour l'entraînement des modèles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97035d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des modèles et leurs hyperparamètres à tester pour la RÉGRESSION\n",
    "models_regression = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],  \n",
    "            'model__solver': ['auto']  \n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs= -1,\n",
    "            max_features='sqrt'  \n",
    "            ),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100],       \n",
    "            'model__max_depth': [15, 20],     \n",
    "            'model__min_samples_split': [5],       \n",
    "            'model__min_samples_leaf': [2]          \n",
    "        } # 2 combinaison\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [50],           \n",
    "            'model__learning_rate': [0.1],    \n",
    "            'model__max_depth': [3, 5],             \n",
    "            'model__min_samples_split': [5]         \n",
    "        } # 2 combinaison\n",
    "    } \n",
    "}\n",
    "\n",
    "print(\"Modèles configurés pour la régression (paramètres simplifiés) :\")\n",
    "for name in models_regression.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement et évaluation des modèles de régression\n",
    "results_regression = {}\n",
    "\n",
    "for model_name, model_info in models_regression.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_info['model'])\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        model_info['params'],\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0  # Aucune sortie pendant l'entraînement\n",
    "    )\n",
    "    \n",
    "    # Entraînement silencieux\n",
    "    grid_search.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Prédictions sur le jeu de test\n",
    "    y_pred = grid_search.predict(X_test_reg)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_reg, y_pred)\n",
    "    r2 = r2_score(y_test_reg, y_pred)\n",
    "    \n",
    "    results_regression[model_name] = {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "\n",
    "# Affichage du résumé final\n",
    "print(\"=\"*80)\n",
    "print(\"RÉSUMÉ DES RÉSULTATS - RÉGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df_reg = pd.DataFrame({\n",
    "    'Modèle': list(results_regression.keys()),\n",
    "    'RMSE': [results_regression[m]['rmse'] for m in results_regression],\n",
    "    'MAE': [results_regression[m]['mae'] for m in results_regression],\n",
    "    'R²': [results_regression[m]['r2'] for m in results_regression]\n",
    "}).sort_values('R²', ascending=False)\n",
    "\n",
    "print(results_df_reg.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# LISTE DES FEATURES UTILISÉES PAR LES MODÈLES DE RÉGRESSION\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURES UTILISÉES POUR LA PRÉDICTION (Modèles de Régression)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'selected_features_smart' in locals() and len(selected_features_smart) > 0:\n",
    "    print(f\"\\n✓ Total : {len(selected_features_smart)} features\")\n",
    "    print(f\"  • Variables numériques   : {len(selected_numeric_smart)}\")\n",
    "    print(f\"  • Variables catégorielles: {len(selected_categorical_smart)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"VARIABLES NUMÉRIQUES ({} features)\".format(len(selected_numeric_smart)))\n",
    "    print(\"─\"*80)\n",
    "    for i, var in enumerate(selected_numeric_smart, 1):\n",
    "        # Afficher avec corrélation si disponible\n",
    "        if 'correlations' in locals() and var in correlations.index:\n",
    "            corr_val = correlations[var]\n",
    "            print(f\"  {i:2d}. {var:60s} | r = {corr_val:+.4f}\")\n",
    "        else:\n",
    "            print(f\"  {i:2d}. {var}\")\n",
    "    \n",
    "    print(\"\\n\" + \"─\"*80)\n",
    "    print(\"VARIABLES CATÉGORIELLES ({} features)\".format(len(selected_categorical_smart)))\n",
    "    print(\"─\"*80)\n",
    "    for i, var in enumerate(selected_categorical_smart, 1):\n",
    "        # Afficher avec Chi-2 si disponible\n",
    "        if 'chi2_df' in locals() and var in chi2_df['Variable'].values:\n",
    "            chi2_val = chi2_df[chi2_df['Variable'] == var]['Chi2_Score'].values[0]\n",
    "            print(f\"  {i:2d}. {var:60s} | Chi2 = {chi2_val:10.2f}\")\n",
    "        else:\n",
    "            print(f\"  {i:2d}. {var}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPORT DE LA LISTE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Créer un dictionnaire structuré pour export\n",
    "    features_export = {\n",
    "        'numeriques': selected_numeric_smart,\n",
    "        'categorielles': selected_categorical_smart,\n",
    "        'total': selected_features_smart\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Variable 'features_export' créée avec la structure complète\")\n",
    "    print(f\"\\nAccès facile :\")\n",
    "    print(f\"  • features_export['numeriques']    → Liste des {len(selected_numeric_smart)} features numériques\")\n",
    "    print(f\"  • features_export['categorielles'] → Liste des {len(selected_categorical_smart)} features catégorielles\")\n",
    "    print(f\"  • features_export['total']         → Liste complète des {len(selected_features_smart)} features\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ Les features n'ont pas encore été sélectionnées.\")\n",
    "    print(\"→ Exécutez d'abord la cellule 'Sélection intelligente des features'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bea485",
   "metadata": {},
   "source": [
    "## Partie 3bis : Modèle Simplifié \"User-Friendly\" - 10 Variables\n",
    "\n",
    "### Test des mêmes 3 modèles avec uniquement les variables facilement collectables\n",
    "\n",
    "**Problématique** : Après avoir identifié les variables les plus pertinentes pour le modèle, on constate que la plupart d'entre elles nécessitent des données difficiles à obtenir pour un utilisateur moyen (ex : coordonnées géographiques précises, caractéristiques techniques spécifiques). Pour rendre le modèle plus accessible et **facile à utiliser**, nous allons créer une version simplifiée utilisant uniquement 10 variables facilement collectables par un utilisateur lambda (ex : surface, type de chauffage, année de construction, etc.).\n",
    "\n",
    "**Objectif** : Comparer les performances d'un modèle simplifié (10 features) vs le modèle complet (36 features) pour évaluer le compromis accessibilité/performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DÉFINITION DES FEATURES USER-FRIENDLY\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODÈLE SIMPLIFIÉ - SÉLECTION DES 10 VARIABLES USER-FRIENDLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Liste des 10 variables facilement collectables\n",
    "user_input_features = [\n",
    "    'surface_habitable_logement',\n",
    "    'periode_construction',\n",
    "    'type_batiment',\n",
    "    'qualite_isolation_enveloppe',\n",
    "    'type_energie_principale_chauffage',\n",
    "    'logement_traversant',\n",
    "    'protection_solaire_exterieure',\n",
    "    'zone_climatique', # définie via code postal\n",
    "    'classe_altitude', # définie via code postal\n",
    "    'apport_interne_saison_chauffe' # Surface_habitable_logement * 15 (en kWh/m²/an)\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ {len(user_input_features)} features sélectionnées\")\n",
    "\n",
    "# Vérifier la disponibilité dans le dataset\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(\"DISPONIBILITÉ ET QUALITÉ DES DONNÉES\")\n",
    "print(f\"{'─'*80}\")\n",
    "\n",
    "available_user_features = []\n",
    "for feat in user_input_features:\n",
    "    if feat in df_work.columns:\n",
    "        missing_pct = df_work[feat].isna().mean() * 100\n",
    "        dtype = df_work[feat].dtype\n",
    "        n_unique = df_work[feat].nunique() if dtype == 'object' else '-'\n",
    "        \n",
    "        status = \"✓\" if missing_pct < 50 else \"⚠\"\n",
    "        print(f\"{status} {feat:50s} | Type: {str(dtype):10s} | Manquants: {missing_pct:5.1f}% | Uniques: {n_unique}\")\n",
    "        \n",
    "        if missing_pct < 50:\n",
    "            available_user_features.append(feat)\n",
    "    else:\n",
    "        print(f\"✗ {feat:50s} | NON DISPONIBLE dans le dataset\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RÉSULTAT : {len(available_user_features)}/{len(user_input_features)} features disponibles et utilisables\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if len(available_user_features) < len(user_input_features):\n",
    "    print(f\"\\n⚠ {len(user_input_features) - len(available_user_features)} features manquantes ou inutilisables\")\n",
    "    missing_features = [f for f in user_input_features if f not in available_user_features]\n",
    "    for feat in missing_features:\n",
    "        print(f\"  → {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PRÉPARATION DES DONNÉES USER-FRIENDLY\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRÉPARATION DU PIPELINE USER-FRIENDLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Séparer les variables numériques et catégorielles\n",
    "user_numeric = [f for f in available_user_features if df_work[f].dtype in ['float64', 'int64']]\n",
    "user_categorical = [f for f in available_user_features if df_work[f].dtype == 'object']\n",
    "\n",
    "print(f\"\\n✓ Classification des features :\")\n",
    "print(f\"  • Numériques   : {len(user_numeric)}\")\n",
    "print(f\"  • Catégorielles: {len(user_categorical)}\")\n",
    "\n",
    "print(f\"\\nVariables numériques :\")\n",
    "for var in user_numeric:\n",
    "    print(f\"  - {var}\")\n",
    "\n",
    "print(f\"\\nVariables catégorielles :\")\n",
    "for var in user_categorical:\n",
    "    n_cat = df_work[var].nunique()\n",
    "    print(f\"  - {var:50s} ({n_cat} catégories)\")\n",
    "\n",
    "# Créer le preprocessor user-friendly\n",
    "user_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), user_numeric),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), user_categorical)\n",
    "    ])\n",
    "\n",
    "print(f\"\\n✓ Preprocessor user-friendly créé avec succès\")\n",
    "\n",
    "# Préparer les données X et y\n",
    "X_user = df_work[available_user_features]\n",
    "y_user = df_work[target_regression]\n",
    "\n",
    "# Division train/test\n",
    "X_train_user, X_test_user, y_train_user, y_test_user = train_test_split(\n",
    "    X_user, y_user, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(f\"DATASETS CRÉÉS\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"Entraînement : {X_train_user.shape}\")\n",
    "print(f\"Test         : {X_test_user.shape}\")\n",
    "print(f\"Features     : {len(available_user_features)}\")\n",
    "print(f\"Target       : {target_regression}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904285f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ENTRAÎNEMENT MODÈLES USER-FRIENDLY\n",
    "# ========================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Définir les modèles avec les mêmes hyperparamètres que le modèle complet\n",
    "models_regression_user = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'model__alpha': [0.01, 0.1, 1.0, 10.0, 100.0],  \n",
    "            'model__solver': ['auto']  \n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(\n",
    "            random_state=42,\n",
    "            n_jobs= 1,\n",
    "            max_features='sqrt'  \n",
    "            ),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100],        \n",
    "            'model__max_depth': [15, 20],     \n",
    "            'model__min_samples_split': [5],        \n",
    "            'model__min_samples_leaf': [2]          \n",
    "        }\n",
    "        \n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [50],           \n",
    "            'model__learning_rate': [0.1],    \n",
    "            'model__max_depth': [3, 5],             \n",
    "            'model__min_samples_split': [5]         \n",
    "        }\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "# Entraînement avec GridSearchCV\n",
    "results_user = {}\n",
    "\n",
    "for model_name, model_config in models_regression_user.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Créer le pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', user_preprocessor),\n",
    "        ('model', model_config['model'])\n",
    "    ])\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        model_config['params'],\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0  # Aucune sortie pendant l'entraînement\n",
    "    )\n",
    "    \n",
    "    # Entraînement silencieux\n",
    "    grid_search.fit(X_train_user, y_train_user)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred_train = grid_search.predict(X_train_user)\n",
    "    y_pred_test = grid_search.predict(X_test_user)\n",
    "    \n",
    "    # Métriques\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_user, y_pred_train))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_user, y_pred_test))\n",
    "    mae_test = mean_absolute_error(y_test_user, y_pred_test)\n",
    "    r2_test = r2_score(y_test_user, y_pred_test)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results_user[model_name] = {\n",
    "        'best_model': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'rmse_train': rmse_train,\n",
    "        'rmse_test': rmse_test,\n",
    "        'mae_test': mae_test,\n",
    "        'r2_test': r2_test,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'training_time': elapsed\n",
    "    }\n",
    "\n",
    "# Affichage du résumé final\n",
    "print(\"=\"*80)\n",
    "print(\"RÉSUMÉ - MODÈLES USER-FRIENDLY (10 FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df_user = pd.DataFrame({\n",
    "    'Modèle': list(results_user.keys()),\n",
    "    'RMSE Train': [results_user[m]['rmse_train'] for m in results_user],\n",
    "    'RMSE Test': [results_user[m]['rmse_test'] for m in results_user],\n",
    "    'MAE Test': [results_user[m]['mae_test'] for m in results_user],\n",
    "    'R² Test': [results_user[m]['r2_test'] for m in results_user],\n",
    "    'Temps (s)': [results_user[m]['training_time'] for m in results_user]\n",
    "}).sort_values('R² Test', ascending=False)\n",
    "\n",
    "print(results_df_user.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VÉRIFICATION RAPIDE DES RÉSULTATS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ DES 3 MODÈLES USER-FRIENDLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'results_user' in locals() and len(results_user) == 3:\n",
    "    print(\"\\n✅ Les 3 modèles ont été entraînés avec succès !\\n\")\n",
    "    \n",
    "    for model_name in ['Ridge', 'RandomForest', 'GradientBoosting']:\n",
    "        if model_name in results_user:\n",
    "            res = results_user[model_name]\n",
    "            print(f\"{'─'*80}\")\n",
    "            print(f\"📊 {model_name}\")\n",
    "            print(f\"{'─'*80}\")\n",
    "            print(f\"  R²   : {res['r2_test']:.4f}\")\n",
    "            print(f\"  RMSE : {res['rmse_test']:,.2f} kWh/m²/an\")\n",
    "            print(f\"  MAE  : {res['mae_test']:,.2f} kWh/m²/an\")\n",
    "            print(f\"  Temps: {res['training_time']:.2f}s\")\n",
    "            print()\n",
    "    \n",
    "    # Identifier le meilleur\n",
    "    best_model_name = max(results_user.keys(), key=lambda k: results_user[k]['r2_test'])\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"🏆 MEILLEUR MODÈLE : {best_model_name} (R² = {results_user[best_model_name]['r2_test']:.4f})\")\n",
    "    print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Les modèles ne sont pas encore entraînés.\")\n",
    "    print(\"   Exécutez la cellule d'entraînement ci-dessus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd87829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# COMPARAISON MODÈLE COMPLET VS USER-FRIENDLY\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON DES PERFORMANCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Créer le DataFrame de comparaison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in ['Ridge', 'RandomForest', 'GradientBoosting']:\n",
    "    # Résultats du modèle complet (36 features)\n",
    "    full_r2 = results_regression[model_name]['r2']\n",
    "    full_rmse = results_regression[model_name]['rmse']\n",
    "    full_mae = results_regression[model_name]['mae']\n",
    "    \n",
    "    # Résultats du modèle user-friendly (10 features)\n",
    "    user_r2 = results_user[model_name]['r2_test']\n",
    "    user_rmse = results_user[model_name]['rmse_test']\n",
    "    user_mae = results_user[model_name]['mae_test']\n",
    "    \n",
    "    # Calcul des rétentions de performance\n",
    "    r2_retention = (user_r2 / full_r2) * 100 if full_r2 != 0 else 0\n",
    "    rmse_increase = ((user_rmse - full_rmse) / full_rmse) * 100\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Modèle': model_name,\n",
    "        'R²_Complet (36f)': full_r2,\n",
    "        'R²_Simplifié (10f)': user_r2,\n",
    "        'Rétention R² (%)': r2_retention,\n",
    "        'RMSE_Complet': full_rmse,\n",
    "        'RMSE_Simplifié': user_rmse,\n",
    "        'Augmentation RMSE (%)': rmse_increase,\n",
    "        'MAE_Complet': full_mae,\n",
    "        'MAE_Simplifié': user_mae\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"─\"*120)\n",
    "print(\"TABLEAU DE COMPARAISON\")\n",
    "print(\"─\"*120)\n",
    "\n",
    "# Affichage formaté\n",
    "for idx, row in df_comparison.iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {row['Modèle'].upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  R² Score :\")\n",
    "    print(f\"    • Complet (36 features)    : {row['R²_Complet (36f)']:.4f}\")\n",
    "    print(f\"    • Simplifié (10 features)  : {row['R²_Simplifié (10f)']:.4f}\")\n",
    "    print(f\"    • Rétention                : {row['Rétention R² (%)']:.1f}%\")\n",
    "    print(f\"\")\n",
    "    print(f\"  RMSE :\")\n",
    "    print(f\"    • Complet                  : {row['RMSE_Complet']:,.2f} kWh/m²/an\")\n",
    "    print(f\"    • Simplifié                : {row['RMSE_Simplifié']:,.2f} kWh/m²/an\")\n",
    "    print(f\"    • Augmentation             : {row['Augmentation RMSE (%)']:+.1f}%\")\n",
    "    print(f\"\")\n",
    "    print(f\"  MAE :\")\n",
    "    print(f\"    • Complet                  : {row['MAE_Complet']:,.2f} kWh/m²/an\")\n",
    "    print(f\"    • Simplifié                : {row['MAE_Simplifié']:,.2f} kWh/m²/an\")\n",
    "\n",
    "# Recommandation automatique\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(f\"RECOMMANDATIONS POUR LE DÉPLOIEMENT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "best_user_model = df_comparison.loc[df_comparison['R²_Simplifié (10f)'].idxmax()]\n",
    "best_retention = best_user_model['Rétention R² (%)']\n",
    "\n",
    "print(f\"\\n🏆 Meilleur modèle simplifié : {best_user_model['Modèle']}\")\n",
    "print(f\"   • R² : {best_user_model['R²_Simplifié (10f)']:.4f}\")\n",
    "print(f\"   • Rétention : {best_retention:.1f}%\")\n",
    "print(f\"   • RMSE : {best_user_model['RMSE_Simplifié']:,.2f} kWh/m²/an\")\n",
    "\n",
    "if best_retention >= 75:\n",
    "    print(f\"\\n✅ EXCELLENT : Rétention ≥ 75%\")\n",
    "    print(f\"   → Recommandation : Utiliser le modèle simplifié\")\n",
    "    print(f\"   → Avantages :\")\n",
    "    print(f\"     • Seulement 5-7 questions utilisateur (3 features auto-dérivées)\")\n",
    "    print(f\"     • Temps de saisie : ~2 minutes\")\n",
    "    print(f\"     • Accessible aux non-experts\")\n",
    "    print(f\"     • Performance conservée à {best_retention:.0f}%\")\n",
    "elif best_retention >= 60:\n",
    "    print(f\"\\n⚠️  BON : Rétention 60-75%\")\n",
    "    print(f\"   → Recommandation : Proposer 2 modes dans l'application\")\n",
    "    print(f\"     • Mode Simple : 10 features (grand public)\")\n",
    "    print(f\"     • Mode Expert : 36 features (professionnels)\")\n",
    "else:\n",
    "    print(f\"\\n❌ INSUFFISANT : Rétention < 60%\")\n",
    "    print(f\"   → Recommandation : Utiliser le modèle complet avec imputation intelligente\")\n",
    "    print(f\"   → Stratégie : Questions progressives + valeurs par défaut\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VISUALISATION DES COMPARAISONS\n",
    "# ========================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comparaison Modèle Complet (36 features) vs Simplifié (10 features)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Graphique en barres - R² Score\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, df_comparison['R²_Complet (36f)'], width, \n",
    "                label='Complet (36f)', color='#2E86AB', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, df_comparison['R²_Simplifié (10f)'], width, \n",
    "                label='Simplifié (10f)', color='#A23B72', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('R² Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Performance R² : Complet vs Simplifié', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df_comparison['Modèle'], fontsize=11)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Graphique en barres - RMSE\n",
    "ax2 = axes[0, 1]\n",
    "bars3 = ax2.bar(x - width/2, df_comparison['RMSE_Complet'], width, \n",
    "                label='Complet (36f)', color='#2E86AB', alpha=0.8)\n",
    "bars4 = ax2.bar(x + width/2, df_comparison['RMSE_Simplifié'], width, \n",
    "                label='Simplifié (10f)', color='#A23B72', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('RMSE (kWh/m²/an)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Erreur RMSE : Complet vs Simplifié', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(df_comparison['Modèle'], fontsize=11)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:,.0f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars4:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:,.0f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Rétention de performance (%)\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['#06A77D' if x >= 75 else '#F77E21' if x >= 60 else '#D62828' \n",
    "          for x in df_comparison['Rétention R² (%)']]\n",
    "bars5 = ax3.bar(df_comparison['Modèle'], df_comparison['Rétention R² (%)'], \n",
    "                color=colors, alpha=0.8)\n",
    "\n",
    "ax3.set_ylabel('Rétention de Performance (%)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Rétention du R² avec 10 features', fontsize=13, fontweight='bold')\n",
    "ax3.axhline(y=75, color='green', linestyle='--', linewidth=2, label='Seuil Excellent (75%)')\n",
    "ax3.axhline(y=60, color='orange', linestyle='--', linewidth=2, label='Seuil Bon (60%)')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "ax3.set_ylim([0, 100])\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for bar in bars5:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 4. Scatter plot - Prédictions GradientBoosting (meilleur modèle)\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Échantillonner pour la visualisation (trop de points sinon)\n",
    "sample_size = min(5000, len(y_test_user))\n",
    "sample_indices = np.random.choice(len(y_test_user), size=sample_size, replace=False)\n",
    "y_test_sample = y_test_user.iloc[sample_indices]\n",
    "y_pred_user_sample = results_user['GradientBoosting']['y_pred_test'][sample_indices]\n",
    "\n",
    "# Pour le modèle complet, utiliser les mêmes valeurs réelles (y_test_user) \n",
    "# mais avec les prédictions complètes sur tout y_test_reg\n",
    "# On utilise seulement le modèle user-friendly pour la comparaison visuelle\n",
    "ax4.scatter(y_test_sample, y_pred_user_sample, alpha=0.3, s=10, \n",
    "            label='Simplifié (10f)', color='#A23B72')\n",
    "\n",
    "# Ligne de référence\n",
    "min_val = min(y_test_sample.min(), y_pred_user_sample.min())\n",
    "max_val = max(y_test_sample.max(), y_pred_user_sample.max())\n",
    "ax4.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Prédiction parfaite')\n",
    "\n",
    "ax4.set_xlabel('Valeurs réelles (kWh/m²/an)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Valeurs prédites (kWh/m²/an)', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('GradientBoosting : Prédictions Réelles vs Prédites', fontsize=13, fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Visualisations générées avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f9f86",
   "metadata": {},
   "source": [
    "## Partie 4 : Modèle de Classification User-Friendly - Prédiction des passoires énergétiques (DPE F et G)\n",
    "\n",
    "**Utilisation des 10 features user-friendly** pour la classification, identiques à celles utilisées en régression.\n",
    "\n",
    "### 4.1 Préparation des données pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PRÉPARATION DES DONNÉES CLASSIFICATION USER-FRIENDLY\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRÉPARATION CLASSIFICATION AVEC 10 FEATURES USER-FRIENDLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Target : passoire énergétique (DPE F ou G)\n",
    "y_classification_user = df_work['passoire']\n",
    "\n",
    "print(f\"\\n✓ Features utilisées : {len(available_user_features)} variables\")\n",
    "print(f\"  • Variables identiques à la régression user-friendly\")\n",
    "print(f\"  • Surface, période construction, type bâtiment, etc.\")\n",
    "\n",
    "# Vérifier la distribution de la cible\n",
    "print(f\"\\n📊 Distribution de la cible 'passoire' :\")\n",
    "print(f\"  • Total observations : {len(y_classification_user):,}\")\n",
    "print(f\"  • Passoires (1)      : {y_classification_user.sum():,} ({y_classification_user.mean()*100:.1f}%)\")\n",
    "print(f\"  • Non-passoires (0)  : {(~y_classification_user.astype(bool)).sum():,} ({(1-y_classification_user.mean())*100:.1f}%)\")\n",
    "\n",
    "# Division train/test avec stratification (même split que pour la régression)\n",
    "X_train_clf_user, X_test_clf_user, y_train_clf_user, y_test_clf_user = train_test_split(\n",
    "    X_user, y_classification_user, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_classification_user\n",
    ")\n",
    "\n",
    "print(f\"\\n{'─'*80}\")\n",
    "print(f\"DATASETS CRÉÉS\")\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"Entraînement : {X_train_clf_user.shape}\")\n",
    "print(f\"Test         : {X_test_clf_user.shape}\")\n",
    "\n",
    "print(f\"\\nDistribution Train :\")\n",
    "print(f\"  • Passoires     : {y_train_clf_user.sum():,} ({y_train_clf_user.mean()*100:.1f}%)\")\n",
    "print(f\"  • Non-passoires : {(~y_train_clf_user.astype(bool)).sum():,} ({(1-y_train_clf_user.mean())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribution Test :\")\n",
    "print(f\"  • Passoires     : {y_test_clf_user.sum():,} ({y_test_clf_user.mean()*100:.1f}%)\")\n",
    "print(f\"  • Non-passoires : {(~y_test_clf_user.astype(bool)).sum():,} ({(1-y_test_clf_user.mean())*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Données prêtes pour la classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da121f8",
   "metadata": {},
   "source": [
    "### 4.2 Optimisation des modèles de classification avec GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ab57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DÉFINITION DES MODÈLES CLASSIFICATION USER-FRIENDLY\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURATION DES MODÈLES DE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Définition des modèles et hyperparamètres (simplifiés pour rapidité)\n",
    "models_classification_user = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'model__C': [0.1, 1.0, 10.0],\n",
    "            'model__penalty': ['l2'],\n",
    "            'model__solver': ['lbfgs']\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42, n_jobs=1),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100],\n",
    "            'model__max_depth': [15, 20],\n",
    "            'model__min_samples_split': [5],\n",
    "            'model__min_samples_leaf': [2],\n",
    "            'model__class_weight': ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [50],\n",
    "            'model__learning_rate': [0.1],\n",
    "            'model__max_depth': [3, 5],\n",
    "            'model__min_samples_split': [5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Modèles configurés :\")\n",
    "for name, config in models_classification_user.items():\n",
    "    n_combinations = 1\n",
    "    for param_values in config['params'].values():\n",
    "        n_combinations *= len(param_values)\n",
    "    print(f\"  • {name:25s} : {n_combinations} combinaisons à tester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77659b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ENTRAÎNEMENT MODÈLES CLASSIFICATION USER-FRIENDLY\n",
    "# ========================================\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "results_classification_user = {}\n",
    "\n",
    "for model_name, model_config in models_classification_user.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Créer le pipeline avec le preprocessor user-friendly\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', user_preprocessor),\n",
    "        ('model', model_config['model'])\n",
    "    ])\n",
    "    \n",
    "    # GridSearchCV avec AUC-ROC comme métrique\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        model_config['params'],\n",
    "        cv=5,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=0  # Aucune sortie pendant l'entraînement\n",
    "    )\n",
    "    \n",
    "    # Entraînement silencieux\n",
    "    grid_search.fit(X_train_clf_user, y_train_clf_user)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred_train = grid_search.predict(X_train_clf_user)\n",
    "    y_pred_test = grid_search.predict(X_test_clf_user)\n",
    "    y_pred_proba_test = grid_search.predict_proba(X_test_clf_user)[:, 1]\n",
    "    \n",
    "    # Métriques\n",
    "    accuracy = accuracy_score(y_test_clf_user, y_pred_test)\n",
    "    precision = precision_score(y_test_clf_user, y_pred_test)\n",
    "    recall = recall_score(y_test_clf_user, y_pred_test)\n",
    "    f1 = f1_score(y_test_clf_user, y_pred_test)\n",
    "    roc_auc = roc_auc_score(y_test_clf_user, y_pred_proba_test)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results_classification_user[model_name] = {\n",
    "        'best_model': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'y_pred_proba_test': y_pred_proba_test,\n",
    "        'training_time': elapsed\n",
    "    }\n",
    "\n",
    "# Affichage du résumé final\n",
    "print(\"=\"*80)\n",
    "print(\"RÉSUMÉ - MODÈLES CLASSIFICATION (10 FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df_clf_user = pd.DataFrame({\n",
    "    'Modèle': list(results_classification_user.keys()),\n",
    "    'Accuracy': [results_classification_user[m]['accuracy'] for m in results_classification_user],\n",
    "    'Precision': [results_classification_user[m]['precision'] for m in results_classification_user],\n",
    "    'Recall': [results_classification_user[m]['recall'] for m in results_classification_user],\n",
    "    'F1-Score': [results_classification_user[m]['f1_score'] for m in results_classification_user],\n",
    "    'ROC-AUC': [results_classification_user[m]['roc_auc'] for m in results_classification_user],\n",
    "    'Temps (s)': [results_classification_user[m]['training_time'] for m in results_classification_user]\n",
    "}).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(results_df_clf_user.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# RÉSUMÉ DES RÉSULTATS CLASSIFICATION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TABLEAU COMPARATIF DES MODÈLES DE CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Créer un DataFrame de comparaison\n",
    "results_clf_df = pd.DataFrame({\n",
    "    'Modèle': list(results_classification_user.keys()),\n",
    "    'Accuracy': [results_classification_user[m]['accuracy'] for m in results_classification_user],\n",
    "    'Precision': [results_classification_user[m]['precision'] for m in results_classification_user],\n",
    "    'Recall': [results_classification_user[m]['recall'] for m in results_classification_user],\n",
    "    'F1-Score': [results_classification_user[m]['f1_score'] for m in results_classification_user],\n",
    "    'ROC-AUC': [results_classification_user[m]['roc_auc'] for m in results_classification_user],\n",
    "    'Temps (s)': [results_classification_user[m]['training_time'] for m in results_classification_user]\n",
    "}).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + results_clf_df.to_string(index=False))\n",
    "\n",
    "# Identifier le meilleur modèle\n",
    "best_clf_model = results_clf_df.iloc[0]['Modèle']\n",
    "best_roc_auc = results_clf_df.iloc[0]['ROC-AUC']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"🏆 MEILLEUR MODÈLE : {best_clf_model}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  • ROC-AUC  : {best_roc_auc:.4f}\")\n",
    "print(f\"  • F1-Score : {results_clf_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"  • Precision: {results_clf_df.iloc[0]['Precision']:.4f}\")\n",
    "print(f\"  • Recall   : {results_clf_df.iloc[0]['Recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3974bf",
   "metadata": {},
   "source": [
    "### 4.3 Visualisations des performances de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VISUALISATION DES PERFORMANCES CLASSIFICATION\n",
    "# ========================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Performances des Modèles de Classification User-Friendly (10 features)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Comparaison des métriques en barres groupées\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(results_clf_df))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "colors_metrics = ['#2E86AB', '#A23B72', '#F18F01', '#06A77D', '#D62828']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    offset = width * (i - 2)\n",
    "    bars = ax1.bar(x + offset, results_clf_df[metric], width, \n",
    "                   label=metric, color=colors_metrics[i], alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Comparaison des Métriques de Classification', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_clf_df['Modèle'], fontsize=11)\n",
    "ax1.legend(fontsize=9, loc='lower right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# 2. Matrice de confusion du meilleur modèle\n",
    "ax2 = axes[0, 1]\n",
    "best_model_name = results_clf_df.iloc[0]['Modèle']\n",
    "y_pred_best = results_classification_user[best_model_name]['y_pred_test']\n",
    "\n",
    "cm = confusion_matrix(y_test_clf_user, y_pred_best)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-passoire', 'Passoire'])\n",
    "disp.plot(ax=ax2, cmap='Blues', colorbar=False)\n",
    "ax2.set_title(f'Matrice de Confusion - {best_model_name}', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Ajouter des annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text_color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax2.text(j, i, f'{cm[i, j]:,}\\n({cm[i, j]/cm.sum()*100:.1f}%)',\n",
    "                ha='center', va='center', color=text_color, fontsize=10, fontweight='bold')\n",
    "\n",
    "# 3. Courbes ROC pour les 3 modèles\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "for model_name in results_classification_user.keys():\n",
    "    y_proba = results_classification_user[model_name]['y_pred_proba_test']\n",
    "    fpr, tpr, _ = roc_curve(y_test_clf_user, y_proba)\n",
    "    roc_auc = results_classification_user[model_name]['roc_auc']\n",
    "    \n",
    "    ax3.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "ax3.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Aléatoire (AUC = 0.5000)')\n",
    "ax3.set_xlabel('Taux de Faux Positifs (FPR)', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Taux de Vrais Positifs (TPR)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Courbes ROC - Comparaison des Modèles', fontsize=13, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Distribution des probabilités prédites (meilleur modèle)\n",
    "ax4 = axes[1, 1]\n",
    "y_proba_best = results_classification_user[best_model_name]['y_pred_proba_test']\n",
    "\n",
    "# Séparer les probabilités selon la vraie classe\n",
    "proba_non_passoire = y_proba_best[y_test_clf_user == 0]\n",
    "proba_passoire = y_proba_best[y_test_clf_user == 1]\n",
    "\n",
    "ax4.hist(proba_non_passoire, bins=50, alpha=0.6, label='Non-passoires (vraies)', \n",
    "         color='#06A77D', edgecolor='black')\n",
    "ax4.hist(proba_passoire, bins=50, alpha=0.6, label='Passoires (vraies)', \n",
    "         color='#D62828', edgecolor='black')\n",
    "\n",
    "ax4.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Seuil = 0.5')\n",
    "ax4.set_xlabel('Probabilité prédite (P(passoire))', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Nombre d\\'observations', fontsize=12, fontweight='bold')\n",
    "ax4.set_title(f'Distribution des Probabilités - {best_model_name}', fontsize=13, fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Visualisations générées avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc576a",
   "metadata": {},
   "source": [
    "### 4.4 Importance des features pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7240c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# IMPORTANCE DES FEATURES (CLASSIFICATION)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPORTANCE DES FEATURES POUR LA CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyser pour RandomForest et GradientBoosting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, model_name in enumerate(['RandomForest', 'GradientBoosting']):\n",
    "    if model_name in results_classification_user:\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Récupérer le modèle entraîné\n",
    "        best_model = results_classification_user[model_name]['best_model']\n",
    "        \n",
    "        # Extraire le modèle de classification du pipeline\n",
    "        clf_model = best_model.named_steps['model']\n",
    "        \n",
    "        # Obtenir les importances\n",
    "        importances = clf_model.feature_importances_\n",
    "        \n",
    "        # Obtenir les noms des features après preprocessing\n",
    "        # Pour simplifier, on utilise les features d'origine\n",
    "        feature_names = available_user_features\n",
    "        \n",
    "        # Créer un DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances[:len(feature_names)]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Graphique\n",
    "        colors = ['#D62828' if i < 3 else '#F18F01' if i < 5 else '#2E86AB' \n",
    "                  for i in range(len(importance_df))]\n",
    "        \n",
    "        ax.barh(importance_df['Feature'], importance_df['Importance'], color=colors, alpha=0.8)\n",
    "        ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'Importance des Features - {model_name}', fontsize=13, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Ajouter les valeurs\n",
    "        for i, (feat, imp) in enumerate(zip(importance_df['Feature'], importance_df['Importance'])):\n",
    "            ax.text(imp, i, f' {imp:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher le top 5 pour chaque modèle\n",
    "for model_name in ['RandomForest', 'GradientBoosting']:\n",
    "    if model_name in results_classification_user:\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"TOP 5 FEATURES - {model_name}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        best_model = results_classification_user[model_name]['best_model']\n",
    "        clf_model = best_model.named_steps['model']\n",
    "        importances = clf_model.feature_importances_\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': available_user_features,\n",
    "            'Importance': importances[:len(available_user_features)]\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        for i, row in importance_df.head(5).iterrows():\n",
    "            print(f\"  {row['Feature']:40s} : {row['Importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"💡 CONCLUSION : Les mêmes features importantes en régression\")\n",
    "print(\"   sont aussi importantes pour la classification des passoires.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae96091",
   "metadata": {},
   "source": [
    "### 4.5 Synthèse finale : Modèles User-Friendly pour la Webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SYNTHÈSE FINALE : MODÈLES POUR LA WEBAPP\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉCAPITULATIF COMPLET - MODÈLES USER-FRIENDLY (10 FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📋 FEATURES UTILISÉES ({len(available_user_features)}) :\")\n",
    "print(\"─\"*80)\n",
    "for i, feat in enumerate(available_user_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"RÉGRESSION : Prédiction de la consommation énergétique\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Meilleur modèle de régression\n",
    "best_reg_model = max(results_user.keys(), key=lambda k: results_user[k]['r2_test'])\n",
    "best_reg_r2 = results_user[best_reg_model]['r2_test']\n",
    "best_reg_rmse = results_user[best_reg_model]['rmse_test']\n",
    "\n",
    "print(f\"\\n🏆 Meilleur modèle : {best_reg_model}\")\n",
    "print(f\"   • R² Score         : {best_reg_r2:.4f} ({best_reg_r2*100:.1f}% de variance expliquée)\")\n",
    "print(f\"   • RMSE             : {best_reg_rmse:,.2f} kWh/m²/an\")\n",
    "print(f\"   • MAE              : {results_user[best_reg_model]['mae_test']:,.2f} kWh/m²/an\")\n",
    "\n",
    "# Comparaison avec modèle complet\n",
    "full_reg_r2 = results_regression[best_reg_model]['r2']\n",
    "retention = (best_reg_r2 / full_reg_r2) * 100\n",
    "print(f\"\\n   ✓ Rétention vs modèle complet (36f) : {retention:.1f}%\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"CLASSIFICATION : Prédiction des passoires énergétiques (DPE F/G)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Meilleur modèle de classification\n",
    "best_clf_model = max(results_classification_user.keys(), \n",
    "                     key=lambda k: results_classification_user[k]['roc_auc'])\n",
    "best_clf_auc = results_classification_user[best_clf_model]['roc_auc']\n",
    "best_clf_f1 = results_classification_user[best_clf_model]['f1_score']\n",
    "\n",
    "print(f\"\\n🏆 Meilleur modèle : {best_clf_model}\")\n",
    "print(f\"   • ROC-AUC          : {best_clf_auc:.4f}\")\n",
    "print(f\"   • F1-Score         : {best_clf_f1:.4f}\")\n",
    "print(f\"   • Accuracy         : {results_classification_user[best_clf_model]['accuracy']:.4f}\")\n",
    "print(f\"   • Precision        : {results_classification_user[best_clf_model]['precision']:.4f}\")\n",
    "print(f\"   • Recall           : {results_classification_user[best_clf_model]['recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"RECOMMANDATIONS POUR LE DÉPLOIEMENT WEBAPP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✅ MODÈLES VALIDÉS POUR PRODUCTION\")\n",
    "print(f\"\\n1️⃣  RÉGRESSION (prédiction consommation) :\")\n",
    "print(f\"    → Modèle : {best_reg_model}\")\n",
    "print(f\"    → Performance : R² = {best_reg_r2:.4f}, RMSE = {best_reg_rmse:,.0f} kWh/m²/an\")\n",
    "print(f\"    → Usage : Estimer la consommation énergétique annuelle\")\n",
    "\n",
    "print(f\"\\n2️⃣  CLASSIFICATION (détection passoires) :\")\n",
    "print(f\"    → Modèle : {best_clf_model}\")\n",
    "print(f\"    → Performance : ROC-AUC = {best_clf_auc:.4f}, F1 = {best_clf_f1:.4f}\")\n",
    "print(f\"    → Usage : Identifier si le logement est une passoire (DPE F ou G)\")\n",
    "\n",
    "print(f\"\\n📝 EXPÉRIENCE UTILISATEUR WEBAPP :\")\n",
    "print(f\"    • Nombre de questions    : 5-7 (3 auto-calculées)\")\n",
    "print(f\"    • Temps de saisie estimé : ~2 minutes\")\n",
    "print(f\"    • Variables auto-dérivées :\")\n",
    "print(f\"      - zone_climatique        → depuis code postal\")\n",
    "print(f\"      - classe_altitude        → depuis code postal\")\n",
    "print(f\"      - apport_interne...      → surface × 15\")\n",
    "\n",
    "print(f\"\\n💾 PROCHAINES ÉTAPES :\")\n",
    "print(f\"    1. Sauvegarder les modèles (joblib.dump)\")\n",
    "print(f\"    2. Créer les fonctions de preprocessing\")\n",
    "print(f\"    3. Développer l'API Flask/FastAPI\")\n",
    "print(f\"    4. Intégrer dans l'interface web\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✅ ANALYSE TERMINÉE - MODÈLES PRÊTS POUR LA WEBAPP\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d5ff4",
   "metadata": {},
   "source": [
    "## Partie 5 : Sauvegarde des Modèles pour la Webapp\n",
    "\n",
    "Les modèles user-friendly (10 features) ont été validés et sont prêts pour le déploiement.\n",
    "Nous allons maintenant les sauvegarder pour utilisation dans la webapp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28468409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SAUVEGARDE DES MODÈLES POUR LA WEBAPP\n",
    "# ========================================\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Créer un timestamp pour versionner les modèles\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAUVEGARDE DES MODÈLES USER-FRIENDLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Créer le dossier de sauvegarde\n",
    "import os\n",
    "models_dir = \"models_pkl\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n📁 Dossier de sauvegarde : {models_dir}/\")\n",
    "\n",
    "# 1. Sauvegarder le meilleur modèle de RÉGRESSION\n",
    "best_reg_model_name = max(results_user.keys(), key=lambda k: results_user[k]['r2_test'])\n",
    "best_reg_model = results_user[best_reg_model_name]['best_model']\n",
    "best_reg_r2 = results_user[best_reg_model_name]['r2_test']\n",
    "best_reg_rmse = results_user[best_reg_model_name]['rmse_test']\n",
    "\n",
    "reg_filename = f\"{models_dir}/regression_{best_reg_model_name.lower()}_{timestamp}.pkl\"\n",
    "joblib.dump(best_reg_model, reg_filename)\n",
    "\n",
    "print(f\"\\n✅ RÉGRESSION sauvegardée :\")\n",
    "print(f\"   • Modèle   : {best_reg_model_name}\")\n",
    "print(f\"   • R²       : {best_reg_r2:.4f}\")\n",
    "print(f\"   • RMSE     : {best_reg_rmse:,.2f} kWh/m²/an\")\n",
    "print(f\"   • Fichier  : {reg_filename}\")\n",
    "\n",
    "# 2. Sauvegarder le meilleur modèle de CLASSIFICATION\n",
    "best_clf_model_name = max(results_classification_user.keys(), \n",
    "                          key=lambda k: results_classification_user[k]['roc_auc'])\n",
    "best_clf_model = results_classification_user[best_clf_model_name]['best_model']\n",
    "best_clf_auc = results_classification_user[best_clf_model_name]['roc_auc']\n",
    "best_clf_f1 = results_classification_user[best_clf_model_name]['f1_score']\n",
    "\n",
    "clf_filename = f\"{models_dir}/classification_{best_clf_model_name.lower()}_{timestamp}.pkl\"\n",
    "joblib.dump(best_clf_model, clf_filename)\n",
    "\n",
    "print(f\"\\n✅ CLASSIFICATION sauvegardée :\")\n",
    "print(f\"   • Modèle   : {best_clf_model_name}\")\n",
    "print(f\"   • ROC-AUC  : {best_clf_auc:.4f}\")\n",
    "print(f\"   • F1-Score : {best_clf_f1:.4f}\")\n",
    "print(f\"   • Fichier  : {clf_filename}\")\n",
    "\n",
    "# 3. Sauvegarder les métadonnées (features utilisées, etc.)\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'features': available_user_features,\n",
    "    'n_features': len(available_user_features),\n",
    "    'regression': {\n",
    "        'model_name': best_reg_model_name,\n",
    "        'r2_score': float(best_reg_r2),\n",
    "        'rmse': float(best_reg_rmse),\n",
    "        'mae': float(results_user[best_reg_model_name]['mae_test']),\n",
    "        'target': 'conso_5_usages_ep'\n",
    "    },\n",
    "    'classification': {\n",
    "        'model_name': best_clf_model_name,\n",
    "        'roc_auc': float(best_clf_auc),\n",
    "        'f1_score': float(best_clf_f1),\n",
    "        'accuracy': float(results_classification_user[best_clf_model_name]['accuracy']),\n",
    "        'precision': float(results_classification_user[best_clf_model_name]['precision']),\n",
    "        'recall': float(results_classification_user[best_clf_model_name]['recall']),\n",
    "        'target': 'passoire'\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'numeric_features': user_numeric,\n",
    "        'categorical_features': user_categorical\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_filename = f\"{models_dir}/metadata_{timestamp}.pkl\"\n",
    "joblib.dump(metadata, metadata_filename)\n",
    "\n",
    "print(f\"\\n✅ MÉTADONNÉES sauvegardées :\")\n",
    "print(f\"   • Fichier : {metadata_filename}\")\n",
    "print(f\"   • Contenu : Features, performances, configuration\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"💾 SAUVEGARDE TERMINÉE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n📦 Fichiers créés :\")\n",
    "print(f\"   1. {reg_filename}\")\n",
    "print(f\"   2. {clf_filename}\")\n",
    "print(f\"   3. {metadata_filename}\")\n",
    "\n",
    "print(f\"\\n💡 Exemple d'intégration :\")\n",
    "print(f\"\"\"\n",
    "import joblib\n",
    "\n",
    "# Charger les modèles\n",
    "model_reg = joblib.load('{reg_filename}')\n",
    "model_clf = joblib.load('{clf_filename}')\n",
    "metadata = joblib.load('{metadata_filename}')\n",
    "\n",
    "# Faire une prédiction\n",
    "# prediction_conso = model_reg.predict(X_new)\n",
    "# prediction_passoire = model_clf.predict(X_new)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f92e93",
   "metadata": {},
   "source": [
    "### Récapitulatif Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa07f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# RÉCAPITULATIF FINAL DU PROJET\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RÉSUMÉ COMPLET DU PROJET - PRÉDICTION DPE HAUTE-SAVOIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "📊 DATASET\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "  • Source            : logements_74.csv\n",
    "  • Observations      : {len(df_work):,} logements en Haute-Savoie\n",
    "  • Variables totales : 236 colonnes initiales\n",
    "  • Features finales  : {len(available_user_features)} variables user-friendly\n",
    "\n",
    "📋 FEATURES SÉLECTIONNÉES (10 variables)\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "for i, feat in enumerate(available_user_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "🎯 MODÈLES DÉVELOPPÉS\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "1️⃣  RÉGRESSION : Prédiction de la consommation énergétique\n",
    "    Target           : conso_5_usages_ep (kWh/m²/an)\n",
    "    Meilleur modèle  : {best_reg_model_name}\n",
    "    Performance      : R² = {best_reg_r2:.4f} (variance expliquée : {best_reg_r2*100:.1f}%)\n",
    "                       RMSE = {best_reg_rmse:,.2f} kWh/m²/an\n",
    "                       MAE = {results_user[best_reg_model_name]['mae_test']:,.2f} kWh/m²/an\n",
    "    \n",
    "    ✓ Rétention vs modèle complet (36f) : {(best_reg_r2 / results_regression[best_reg_model_name]['r2'])*100:.1f}%\n",
    "\n",
    "2️⃣  CLASSIFICATION : Détection des passoires énergétiques (DPE F/G)\n",
    "    Target           : passoire (0=Non, 1=Oui)\n",
    "    Meilleur modèle  : {best_clf_model_name}\n",
    "    Performance      : ROC-AUC = {best_clf_auc:.4f}\n",
    "                       F1-Score = {best_clf_f1:.4f}\n",
    "                       Accuracy = {results_classification_user[best_clf_model_name]['accuracy']:.4f}\n",
    "                       Precision = {results_classification_user[best_clf_model_name]['precision']:.4f}\n",
    "                       Recall = {results_classification_user[best_clf_model_name]['recall']:.4f}\n",
    "\n",
    "💾 SAUVEGARDE\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "  • Dossier         : {models_dir}\n",
    "  • Modèle régression    : ✅ Sauvegardé\n",
    "  • Modèle classification : ✅ Sauvegardé\n",
    "  • Métadonnées     : ✅ Sauvegardées\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"✅ PROJET TERMINÉ\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
